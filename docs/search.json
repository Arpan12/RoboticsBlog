[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RoboticsBlog",
    "section": "",
    "text": "Python and Numpy cheat sheet for DL\n\n\n\n\n\n\n\nDL\n\n\nCode\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nJun 10, 2023\n\n\nArpan Pallar\n\n\n\n\n\n\n  \n\n\n\n\nPPO Implementation for Coders\n\n\n\n\n\n\n\nRL\n\n\nCode\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nJun 10, 2023\n\n\nArpan Pallar\n\n\n\n\n\n\n  \n\n\n\n\nTensorFLow and Pytorch Cheat Sheet for RL\n\n\n\n\n\n\n\nRL\n\n\nCode\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nJun 10, 2023\n\n\nArpan Pallar\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/PPO Implementation for Coders/index.html",
    "href": "posts/PPO Implementation for Coders/index.html",
    "title": "PPO Implementation for Coders",
    "section": "",
    "text": "PPO is state of the art algorithm for DeepRL. We ll see how this can be implemented in code.\nWe wont go in depth into theory but focus on implementation."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html",
    "title": "TensorFLow and Pytorch Cheat Sheet for RL",
    "section": "",
    "text": "Create a pytorch Neural Network"
  },
  {
    "objectID": "posts/Python,Numpy cheat sheet for DL/index.html",
    "href": "posts/Python,Numpy cheat sheet for DL/index.html",
    "title": "Python and Numpy cheat sheet for DL",
    "section": "",
    "text": "A cheat sheet for common numpy and Python APIs you would see and require for deep learning based programming for robotics"
  },
  {
    "objectID": "posts/Python,Numpy cheat sheet for DL/index.html#np.zeros",
    "href": "posts/Python,Numpy cheat sheet for DL/index.html#np.zeros",
    "title": "Python and Numpy cheat sheet for DL",
    "section": "np.zeros",
    "text": "np.zeros\nnumpy.zeros(shape, dtype=float, order=‘C’, *, like=None)\nReturn a new array of given shape and type, filled with zeros.\n\n\nParameters:\n\n\nshapeint or tuple of ints\n\nShape of the new array, e.g., (2, 3) or 2.\n\ndtypedata-type, optional\n\nThe desired data-type for the array, e.g., numpy.int8. Default is numpy.float64.\n\norder{‘C’, ‘F’}, optional, default: ‘C’\n\nWhether to store multi-dimensional data in row-major (C-style) or column-major (Fortran-style) order in memory.\n\nlikearray_like, optional\n\nReference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.\n\n\n\n\nnp.zeros(5)\narray([ 0.,  0.,  0.,  0.,  0.])  # default is numpy.float64\n&gt;&gt;&gt;np.zeros((5,), dtype=int) #if you want to create a 1D array\narray([0, 0, 0, 0, 0])\n\n&gt;&gt;&gt;np.zeros((5,1))\narray([[0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.]])\n       \n       \n&gt;&gt;&gt;s = (2,2)\n&gt;&gt;&gt;np.zeros(s)\narray([[ 0.,  0.],\n       [ 0.,  0.]]\n       \n#Multi-dim array \n&gt;&gt; np.zeros((4,3,2))  #a row of 2 element repeated 3 times-&gt; 3*2 array-&gt;repeated 4 times to give 4*3*2 array\n\narray([[[0., 0.],\n        [0., 0.],\n        [0., 0.]],\n\n       [[0., 0.],\n        [0., 0.],\n        [0., 0.]],\n\n       [[0., 0.],\n        [0., 0.],\n        [0., 0.]],\n\n       [[0., 0.],\n        [0., 0.],\n        [0., 0.]]])"
  },
  {
    "objectID": "posts/Python,Numpy cheat sheet for DL/index.html#get-and-set-numpy-array-value",
    "href": "posts/Python,Numpy cheat sheet for DL/index.html#get-and-set-numpy-array-value",
    "title": "Python and Numpy cheat sheet for DL",
    "section": "Get and Set Numpy Array Value",
    "text": "Get and Set Numpy Array Value\nx = np.zeros((4,3,2))\n#just like you would do in Python list or C array\nx[3][1][0]=1\nNegative Indexing"
  },
  {
    "objectID": "posts/Python,Numpy cheat sheet for DL/index.html#np.zeros_like",
    "href": "posts/Python,Numpy cheat sheet for DL/index.html#np.zeros_like",
    "title": "Python and Numpy cheat sheet for DL",
    "section": "np.zeros_like",
    "text": "np.zeros_like\n\nnumpy.zeros_like(a, dtype=None, order=‘K’, subok=True, shape=None)[source]\n\nReturn an array of zeros with the same shape and type as a given array.\n\nParameters:\n\n\naarray_like\n\nThe shape and data-type of a define these same attributes of the returned array.\n\ndtypedata-type, optional\n\nOverrides the data type of the result.\n\n\n\n\n\n\n&gt;&gt;&gt;x = np.arange(6)}\n&gt;&gt;&gt;x = x.reshape((2, 3))\n&gt;&gt;&gt;x\narray([[0, 1, 2],\n       [3, 4, 5]])\n&gt;&gt;&gt;np.zeros_like(x)\narray([[0, 0, 0],\n       [0, 0, 0]])"
  },
  {
    "objectID": "posts/Python,Numpy cheat sheet for DL/index.html#np.arange",
    "href": "posts/Python,Numpy cheat sheet for DL/index.html#np.arange",
    "title": "Python and Numpy cheat sheet for DL",
    "section": "np.arange",
    "text": "np.arange\n&gt;&gt;&gt;np.arange(6) # 1D array\narray([0, 1, 2, 3, 4, 5])"
  },
  {
    "objectID": "posts/Python,Numpy cheat sheet for DL/index.html#np.reshape",
    "href": "posts/Python,Numpy cheat sheet for DL/index.html#np.reshape",
    "title": "Python and Numpy cheat sheet for DL",
    "section": "np.reshape",
    "text": "np.reshape\n\nnumpy.reshape(a, newshape, order=‘C’)\n\nGives a new shape to an array without changing its data.\n\nParameters:\n\n\naarray_like\n\nArray to be reshaped.\n\nnewshapeint or tuple of ints\n\nThe new shape should be compatible with the original shape. If an integer, then the result will be a 1-D array of that length. One shape dimension can be -1. In this case, the value is inferred from the length of the array and remaining dimensions.\n\n\n\n\n&gt;&gt;&gt;a = np.arange(6).reshape((3, 2))}\n&gt;&gt;&gt;a       \n#notice it is row wise. It makes easier to visualize if you put all the element \n#in 1D row array. Then take start row wise rearrangement  \n\narray([[0, 1],\n       [2, 3],\n       [4, 5]])\n\n&gt;&gt;&gt;np.reshape(a, (2, 3)) # C-like index ordering\narray([[0, 1, 2],\n       [3, 4, 5]])\n       \n#there can be one -1 index meaning its dimention are infered from number of elements and remaining specified shape\n&gt;&gt;&gt;np.reshape(a, (3,-1))       # the unspecified value is inferred to be 2\narray([[1, 2],\n       [3, 4],\n       [5, 6]])"
  },
  {
    "objectID": "posts/Python,Numpy cheat sheet for DL/index.html#np.ones",
    "href": "posts/Python,Numpy cheat sheet for DL/index.html#np.ones",
    "title": "Python and Numpy cheat sheet for DL",
    "section": "np.ones:",
    "text": "np.ones:\neverything similar to np.zeros\nnp.ones((2, 1))\narray([[1.],\n       [1.]])"
  },
  {
    "objectID": "posts/Python,Numpy cheat sheet for DL/index.html#np-array-slices",
    "href": "posts/Python,Numpy cheat sheet for DL/index.html#np-array-slices",
    "title": "Python and Numpy cheat sheet for DL",
    "section": "np array slices",
    "text": "np array slices\nThe syntax of Python NumPy slicing is [start : stop : step]\n\nStart : This index by default considers as ‘0’\nstop : This index considers as a length of the array.\nstep : By default it is considered as ‘1’.\n\n#this is another way to create a numpy array\n&gt;&gt;&gt;arr = np.array([3, 5, 7, 9, 11, 15, 18, 22]) \n&gt;&gt;&gt; arr2 = arr[:5]\n# [ 3  5  7  9 11]\n&gt;&gt;&gt;arr2 = arr[-4:-2]\n# [11 15]\n&gt;&gt;&gt;arr2 = arr[::3]\n#[ 3  9 18]\n\n\narr = np.array([[3, 5, 7, 9, 11],\n               [2, 4, 6, 8, 10]])\n               \n\n# Use slicing a 2-D arrays\narr2 = arr[1:,1:3]  #2D array remains 2D array\n[[4 6]]\n\narr2 = arr[1,1:3] #2D array remains 1D array\n\n#now try solving this before looking at the solution\narr = np.array([[[3, 5, 7, 9, 11],\n                 [2, 4, 6, 8, 10]],\n                [[5, 7, 8, 9, 2],\n                 [7, 2, 3, 6, 7]]])\n               \narr2 = arr[0,1,0:2]\n[2 4]"
  },
  {
    "objectID": "posts/Python,Numpy cheat sheet for DL/index.html#np.eye",
    "href": "posts/Python,Numpy cheat sheet for DL/index.html#np.eye",
    "title": "Python and Numpy cheat sheet for DL",
    "section": "np.eye:",
    "text": "np.eye:\n\nnumpy.eye(N, M=None, k=0, dtype=&lt;class ‘float’&gt;, order=‘C’, *, like=None)\n\nReturn a 2-D array with ones on the diagonal and zeros elsewhere.\n\nParameters:\n\n\nNint\n\nNumber of rows in the output.\n\nMint, optional\n\nNumber of columns in the output. If None, defaults to N.\n\nkint, optional\n\nIndex of the diagonal: 0 (the default) refers to the main diagonal, a positive value refers to an upper diagonal, and a negative value to a lower diagonal.\n\ndtypedata-type, optional\n\nData-type of the returned array.\n\n\n\n\n\n\n&gt;&gt;&gt;np.eye(2, dtype=int)\narray([[1, 0],\n       [0, 1]])\n&gt;&gt;&gt; np.eye(3, k=1)\narray([[0.,  1.,  0.],\n       [0.,  0.,  1.],\n       [0.,  0.,  0.]])"
  },
  {
    "objectID": "posts/Python,Numpy cheat sheet for DL/index.html#reversedrangen",
    "href": "posts/Python,Numpy cheat sheet for DL/index.html#reversedrangen",
    "title": "Python and Numpy cheat sheet for DL",
    "section": "reversed(range(n))",
    "text": "reversed(range(n))\niterate in reverse ie. n-1,n-2,…0"
  },
  {
    "objectID": "posts/Python,Numpy cheat sheet for DL/index.html#np.random.shuffle",
    "href": "posts/Python,Numpy cheat sheet for DL/index.html#np.random.shuffle",
    "title": "Python and Numpy cheat sheet for DL",
    "section": "np.random.shuffle",
    "text": "np.random.shuffle\n\nrandom.shuffle(x)\n\nModify a sequence in-place by shuffling its contents.\nThis function only shuffles the array along the first axis of a multi-dimensional array. The order of sub-arrays is changed but their contents remains the same.\n\n\narr = np.arange(10)\n&gt;&gt;&gt; np.random.shuffle(arr)\n&gt;&gt;&gt; arr\n[1 7 5 2 9 4 3 6 0 8] # random\n\n# works for multi-D array also\n\narr = np.arange(9).reshape((3, 3))\nnp.random.shuffle(arr)\narr\narray([[3, 4, 5], # random\n       [6, 7, 8],\n       [0, 1, 2]])"
  },
  {
    "objectID": "posts/Python,Numpy cheat sheet for DL/index.html#numpy-axes",
    "href": "posts/Python,Numpy cheat sheet for DL/index.html#numpy-axes",
    "title": "Python and Numpy cheat sheet for DL",
    "section": "Numpy Axes",
    "text": "Numpy Axes\nfor a 2D array remember this figure\n\n&gt;&gt;&gt; np_array_2d = np.arange(0, 6).reshape([2,3])\n[[0 1 2]\n [3 4 5]]\n\n&gt;&gt;&gt;np.sum(np_array_2d, axis = 0) \narray([3, 5, 7])\n\n&gt;&gt;&gt;np.sum(np_array_2d, axis = 1) #2d array become 1D in either case \n\narray([3, 12])   # note its not array([[3],[12]])\n\n&gt;&gt;&gt; np_array_1s = np.array([[1,1,1],[1,1,1]])\narray([[1, 1, 1],\n       [1, 1, 1]])\n&gt;&gt;&gt; np_array_9s = np.array([[9,9,9],[9,9,9]])\narray([[9, 9, 9],\n       [9, 9, 9]])\n\n&gt;&gt;&gt; np.concatenate([np_array_1s, np_array_9s], axis = 0)\narray([[1, 1, 1],\n       [1, 1, 1],\n       [9, 9, 9],\n       [9, 9, 9]])\n\n&gt;&gt;&gt; np.concatenate([np_array_1s, np_array_9s], axis = 1)\narray([[1, 1, 1, 9, 9, 9],\n       [1, 1, 1, 9, 9, 9]])\n      \n# Be careful with 1D arrays are different. there is only 1 axis ie axis 0\n\n&gt;&gt;&gt; np_array_1s_1dim = np.array([1,1,1])\n&gt;&gt;&gt; np_array_9s_1dim = np.array([9,9,9])\n\n&gt;&gt;&gt; np.concatenate([np_array_1s_1dim, np_array_9s_1dim], axis = 0)\n\narray([1, 1, 1, 9, 9, 9])"
  },
  {
    "objectID": "posts/PPO Implementation for Coders/index.html#implementation",
    "href": "posts/PPO Implementation for Coders/index.html#implementation",
    "title": "PPO Implementation for Coders",
    "section": "Implementation",
    "text": "Implementation\n\nMake Actor and Critic NN\nactor NN: takes in a state(you need to make a tensor out of your state.Cant pass a list or np array) and spits out a Categorical object containing probs(because the last layer was softmax on action_dim)\nfor eg: in our case our observation is (4,) python list\n&gt;&gt;&gt; print(f\"observation= {observation}\")\nobservation= [ 0.02916372  0.02608052  0.01869606 -0.00384168]\nwe need to convert them to Tensor before passing them to NN\n&gt;&gt;&gt; state = T.tensor([observation],dtype=T.float).to(self.actor.device)\n&gt;&gt;&gt; print(f\"state= {state}\")\nstate= tensor([[ 0.0292,  0.0261,  0.0187, -0.0038]], device='cuda:0')\nwe get a tensor with Probability dist from NN(last layer was softmax). We then get an action by sampling the dist object\n&gt;&gt;&gt; dist = self.actor(state)\n&gt;&gt;&gt; print(f\"dist {dist} {dist.probs}\")\ndist Categorical(probs: torch.Size([1, 2])) tensor([[0.4913, 0.5087]], device='cuda:0', grad_fn=&lt;DivBackward0&gt;)\n\n&gt;&gt;&gt; action = dist.sample()\n&gt;&gt;&gt; print(f\"action = {action}\")\naction = tensor([1], device='cuda:0')\n\n# Retrieve action from action tensor and log_probs\n# squeeze removes all axises having value 1. item() returns a standard python float\n&gt;&gt;&gt;  probs = T.squeeze(dist.log_prob(action)).item()\n&gt;&gt;&gt;  action = T.squeeze(action).item()\n&gt;&gt;&gt; print(f\"log prob {dist.log_prob(action)} squeezed = {T.squeeze(dist.log_prob(action))} Probs = {probs}\")\nlog prob tensor([-0.7533], device='cuda:0', grad_fn=&lt;SqueezeBackward1&gt;) squeezed = -0.753328263759613 Probs = -0.753328263759613\nThe code For Actor class. It derives from Torch.nn.Module\nclass AgentNetwork(nn.Module):\n    def __init__(self,input_dims,action_dim,lr,layer1=256,layer2=256,weight_file='weightFiles/ppo_discrete'):\n        super(AgentNetwork,self).__init__()\n        self.checkpoint_file = os.path.join(weight_file,'ppo_actor_weight')\n        #TOCHECK: *input_dims vs input_dims\n        self.actor = nn.Sequential(\n                nn.Linear(*input_dims,layer1),\n                nn.ReLU(),\n                nn.Linear(layer1,layer2),\n                nn.ReLU(),\n                nn.Linear(layer2,action_dim),\n                nn.Softmax(dim=-1)               \n        )\n        \n        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n        self.to(self.device)\n    \n    def forward(self,state):\n        dist = self.actor(state)\n        #TOCHECK: what does categorical do\n        dist = Categorical(dist)\n        return dist\n    \n    def save_checkpoint(self):\n        T.save(self.state_dict(),self.checkpoint_file)\n    \n    def load_checkpoint(self):\n        self.load_state_dict(T.load(self.checkpoint_file))\nCritic NN: takes in a state(tensor) and returns a tensor containing a single float corresponding to Value of that state\n&gt;&gt;&gt; state = T.tensor([observation],dtype=T.float).to(self.actor.device)\n&gt;&gt;&gt; value = self.critic(state)\n&gt;&gt;&gt; print(f\"value {value}\")\nvalue tensor([[-0.0977]], device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;)\nCode for Critic Class\n class CriticNetwork(nn.Module):\n    def __init__(self,input_dims,lr,layer1=256,layer2=256,weight_file='weightFiles/ppo_discrete'):\n        super(CriticNetwork,self).__init__()\n        self.checkpoint_file = os.path.join(weight_file,'ppo_critic_weight')\n        self.critic = nn.Sequential(\n                nn.Linear(*input_dims,layer1),\n                nn.ReLU(),\n                nn.Linear(layer1,layer2),\n                nn.ReLU(),\n                nn.Linear(layer2,1)\n        )\n        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n        self.to(self.device)\n        \n    def forward(self,state):\n        value = self.critic(state)\n        return value\n    \n    def save_checkpoint(self):\n        T.save(self.state_dict(),self.checkpoint_file)\n    \n    def load_checkpoint(self):\n        self.load_state_dict(T.load(self.checkpoint_file))"
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#specify-optimizer-in-pytorch",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#specify-optimizer-in-pytorch",
    "title": "TensorFLow and Pytorch Cheat Sheet for RL",
    "section": "Specify Optimizer in Pytorch",
    "text": "Specify Optimizer in Pytorch"
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#theory-behind-assigning-tensor-to-device",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#theory-behind-assigning-tensor-to-device",
    "title": "TensorFLow and Pytorch Cheat Sheet for RL",
    "section": "Theory behind assigning tensor to device",
    "text": "Theory behind assigning tensor to device"
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#save-and-load-model-checkpoint-file",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#save-and-load-model-checkpoint-file",
    "title": "TensorFLow and Pytorch Cheat Sheet for RL",
    "section": "Save and Load Model checkpoint File",
    "text": "Save and Load Model checkpoint File\nIf the NN class inherits from torch.nn.Module,then you can get the parameters/weights directly from self.state_dict() and save using torch.save()\nT.save(self.state_dict(),self.checkpoint_file)"
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#distribution",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#distribution",
    "title": "TensorFLow and Pytorch Cheat Sheet for RL",
    "section": "distribution",
    "text": "distribution"
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#t.squeeze",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#t.squeeze",
    "title": "TensorFLow and Pytorch Cheat Sheet for RL",
    "section": "T.squeeze()",
    "text": "T.squeeze()\ntorch.squeeze(input, dim=None) → Tensor Returns a tensor with all specified dimensions of input of size 1 removed.\nFor example, if input is of shape: (A×1×B×C×1×D) then the input.squeeze() will be of shape: (A×B×C×D).\nWhen dim is given, a squeeze operation is done only in the given dimension(s). If input is of shape: (A×1×B), squeeze(input, 0) leaves the tensor unchanged, but squeeze(input, 1) will squeeze the tensor to the shape (A×B). Parameters: input (Tensor) – the input tensor.\ndim (int or tuple of ints, optional) –if given, the input will be squeezed only in the specified dimensions\n&gt;&gt;&gt; x = torch.zeros(2, 1, 2, 1, 2) \n&gt;&gt;&gt; x.size() \ntorch.Size([2, 1, 2, 1, 2]) \n&gt;&gt;&gt; y = torch.squeeze(x) \n&gt;&gt;&gt; y.size() \ntorch.Size([2, 2, 2]) \n&gt;&gt;&gt; y = torch.squeeze(x, 0) \n&gt;&gt;&gt; y.size() \ntorch.Size([2, 1, 2, 1, 2]) \n&gt;&gt;&gt; y = torch.squeeze(x, 1) \n&gt;&gt;&gt; y.size() \ntorch.Size([2, 2, 1, 2]) \n&gt;&gt;&gt; y = torch.squeeze(x, (1, 2, 3)) \ntorch.Size([2, 2, 2])"
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#numpy-to-tensor",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#numpy-to-tensor",
    "title": "TensorFLow and Pytorch Cheat Sheet for RL",
    "section": "numpy to Tensor",
    "text": "numpy to Tensor\n&gt;&gt;&gt; print(f\"observation= {observation}\") \nobservation= [ 0.02916372 0.02608052 0.01869606 -0.00384168]\n&gt;&gt;&gt; state = T.tensor([observation],dtype=T.float).to(self.actor.device)\n&gt;&gt;&gt; print(f\"state= {state}\")\nstate= tensor([[ 0.0292,  0.0261,  0.0187, -0.0038]], device='cuda:0')\n\n\n# Another way. convert from numpy to PyTorch\nnp_array = np.ones((2,3))\npytorch_tensor = torch.from_numpy(np_array)\n# convert from PyTorch to numpy\npt_tensor = torch.rand(2,3)\nnumpy_array = pt_tensor.numpy()\n# they share same underlying memory. So, changes to one shall be reflected on the other. \n# for ex : pt_tensor update results in an update to numpy_array."
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#optimizer.zero_grad",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#optimizer.zero_grad",
    "title": "TensorFLow and Pytorch Cheat Sheet for RL",
    "section": "optimizer.zero_grad",
    "text": "optimizer.zero_grad"
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#total_loss.backward",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#total_loss.backward",
    "title": "TensorFLow and Pytorch Cheat Sheet for RL",
    "section": "total_loss.backward",
    "text": "total_loss.backward"
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#optimizer.step",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#optimizer.step",
    "title": "TensorFLow and Pytorch Cheat Sheet for RL",
    "section": "optimizer.step",
    "text": "optimizer.step"
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#pytorch-softmax-distribution",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#pytorch-softmax-distribution",
    "title": "TensorFLow and Pytorch Cheat Sheet for RL",
    "section": "PyTorch SoftMax / Distribution",
    "text": "PyTorch SoftMax / Distribution\nwe get a tensor with Probability dist from NN(last layer was softmax). We then get an action by sampling the dist object\n&gt;&gt;&gt; dist = self.actor(state)\n&gt;&gt;&gt; print(f\"dist {dist} {dist.probs}\")\ndist Categorical(probs: torch.Size([1, 2])) tensor([[0.4913, 0.5087]], device='cuda:0', grad_fn=&lt;DivBackward0&gt;)\n\n&gt;&gt;&gt; action = dist.sample()\n&gt;&gt;&gt; print(f\"action = {action}\")\naction = tensor([1], device='cuda:0')"
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#optimizer-in-pytorch",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#optimizer-in-pytorch",
    "title": "TensorFLow and Pytorch Cheat Sheet for RL",
    "section": "Optimizer in Pytorch",
    "text": "Optimizer in Pytorch"
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#torch.tensor",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#torch.tensor",
    "title": "TensorFLow and Pytorch Cheat Sheet for RL",
    "section": "Torch.Tensor",
    "text": "Torch.Tensor\nPyTorch provides torch.Tensor to represent a multi-dimensional array containing elements of a single data type\nIt is basically the same as a numpy array: it does not know anything about deep learning or computational graphs or gradients, and is just a generic n-dimensional array to be used for arbitrary numeric computation. The biggest difference between a numpy array and a PyTorch Tensor is that a PyTorch Tensor can run on either CPU or GPU. To run operations on the GPU, just cast the Tensor to a cuda datatype.\nThere are three attributes : torch.dtype, torch.device and torch.layout\na = torch.rand(2,2, device='cpu')\n# transfer tensor created on cpu to gpu accessible memory.\nif torch.cuda.is_available():\n      device = torch.device('cuda') # create a device handle\n       a = a.to(device) # pass device handle created."
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#copying-tensor",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#copying-tensor",
    "title": "TensorFLow and Pytorch Cheat Sheet for RL",
    "section": "Copying Tensor",
    "text": "Copying Tensor"
  },
  {
    "objectID": "posts/PPO Implementation for Coders/index.html#batches-and-how-to-generate-them",
    "href": "posts/PPO Implementation for Coders/index.html#batches-and-how-to-generate-them",
    "title": "PPO Implementation for Coders",
    "section": "Batches and How to generate them",
    "text": "Batches and How to generate them\nWe need to take all the elements in the memory and create batches out of them.\nGOAL\n1.The elements should not repeat from one batch to another\n2.There should be any correlation between elements of same batch.\nHere is how we achieve that in code\n  def generate_batches(self):\n        n_states = len(self.states)\n        batches=[]\n        i=0\n        indices = np.arange(n_states,dtype = np.int64)\n        np.random.shuffle(indices)\n        for i in range(n_states):\n            batches.append(indices[i:i+self.batch_size])\n            i+=self.batch_size\n        return np.array(self.states),\\\n                np.array(self.actions),\\\n                np.array(self.probs), \\\n                np.array(self.vals),\\\n                np.array(self.rewards), \\\n                np.array(self.dones),\\\n                batches\n#Note here only batch index get suffled. We need to have the actual order of states,actions,probs,vals,rewards and done so that we can calculate advatage of each state later"
  },
  {
    "objectID": "posts/PPO Implementation for Coders/index.html#compute-the-advantage",
    "href": "posts/PPO Implementation for Coders/index.html#compute-the-advantage",
    "title": "PPO Implementation for Coders",
    "section": "compute the advantage",
    "text": "compute the advantage\nWe know that advantage of a state is given by following equation\n\\(A_t = \\sum_{t=k}^N (\\lambda *\\mu)^{t-k}*( Q(x_n,u_n)-V(x_n))\\)\nHere is how we achieve that in code\nstate_arr,action_arr,probs_arr,vals_arr, \\\n    rewards_arr,dones_arr,batches = self.memory.generate_batches()\n            \nadvantages=np.zeros_like(rewards_arr)\n            \nfor t in reversed(range(len(state_arr)-1)):\n    advantages[t] = rewards_arr[t]+self.gamma*vals_arr[t+1]*(1-int(dones_arr[t]))-vals_arr[t] + self.gamma*self.lambda_factor*advantages[t+1]\n                \nadvantages = T.tensor(advantages).to(self.actor.device)"
  },
  {
    "objectID": "posts/PPO Implementation for Coders/index.html#loss-function",
    "href": "posts/PPO Implementation for Coders/index.html#loss-function",
    "title": "PPO Implementation for Coders",
    "section": "loss function",
    "text": "loss function\nfor actor loss,we know that we need to minimize\n\\(min_\\theta E[\\sum_{n=0}^N min(A_n\\ \\frac{\\pi(u_n|x_n,Theta)}{\\pi(u_n| x_n,\\theta_{old}},clip(\\frac{\\pi(u_n|x_n,Theta)}{\\pi(u_n| x_n,\\theta_{old}},1-\\epsilon,1+\\epsilon)A_n)]\\)\nso we can find gradient of this function with respect to our weights \\(\\Theta\\) and do a 1 step gradient descent\nThis can be done in Code by\n states = T.tensor(state_arr[batch],dtype = T.float).to(self.actor.device)\nactions = T.tensor(action_arr[batch],dtype = T.float).to(self.actor.device)\nold_probs = T.tensor(probs_arr[batch],dtype = T.float).to(self.actor.device)\ndist = self.actor(states)\nnew_probs = dist.log_prob(actions)\n#TOCHECK: what do exp() do\n                \nprob_ratio = new_probs.exp()/old_probs.exp()\n                \nweighted_prob = advantages[batch]*(prob_ratio)\n                \nweighted_clipped_probs = T.clamp(prob_ratio,1-self.policy_clip,1+self.policy_clip)*advantages[batch]\n                \nactor_loss = - T.min(weighted_clipped_probs,weighted_prob).mean()\nfor Critic loss,we know that we need to minimize the error of values of our state \\((V_{des} - V_{pred})^2\\)\nwhere\n\\(V_{des} = a_t+value_{theta\\_old}\\)\n\\(V_{pred} = value \\ from \\ critic \\ NN\\)\nThis is done in code as\ncritic_values = T.squeeze(critic_values)\n                \ndesired_state_values = advantages[batch]+values[batch]\ncritic_loss = (desired_state_values-critic_values)**2\ncritic_loss = critic_loss.mean()\nThen We calculate the total loss and do 1 step gradient descent as\n total_loss = actor_loss+critic_loss*0.5\n                \nself.actor.optimizer.zero_grad()\nself.critic.optimizer.zero_grad()\ntotal_loss.backward()\nself.actor.optimizer.step()\nself.critic.optimizer.step()"
  },
  {
    "objectID": "posts/PPO Implementation for Coders/index.html#make-actor-and-critic-nn",
    "href": "posts/PPO Implementation for Coders/index.html#make-actor-and-critic-nn",
    "title": "PPO Implementation for Coders",
    "section": "Make Actor and Critic NN",
    "text": "Make Actor and Critic NN\nactor NN: takes in a state(you need to make a tensor out of your state.Cant pass a list or np array) and spits out a Categorical object containing probs(because the last layer was softmax on action_dim)\nfor eg: in our case our observation is (4,) python list\n&gt;&gt;&gt; print(f\"observation= {observation}\")\nobservation= [ 0.02916372  0.02608052  0.01869606 -0.00384168]\nwe need to convert them to Tensor before passing them to NN\n&gt;&gt;&gt; state = T.tensor([observation],dtype=T.float).to(self.actor.device)\n&gt;&gt;&gt; print(f\"state= {state}\")\nstate= tensor([[ 0.0292,  0.0261,  0.0187, -0.0038]], device='cuda:0')\nwe get a tensor with Probability dist from NN(last layer was softmax). We then get an action by sampling the dist object\n&gt;&gt;&gt; dist = self.actor(state)\n&gt;&gt;&gt; print(f\"dist {dist} {dist.probs}\")\ndist Categorical(probs: torch.Size([1, 2])) tensor([[0.4913, 0.5087]], device='cuda:0', grad_fn=&lt;DivBackward0&gt;)\n\n&gt;&gt;&gt; action = dist.sample()\n&gt;&gt;&gt; print(f\"action = {action}\")\naction = tensor([1], device='cuda:0')\n\n# Retrieve action from action tensor and log_probs\n# squeeze removes all axises having value 1. item() returns a standard python float\n&gt;&gt;&gt;  probs = T.squeeze(dist.log_prob(action)).item()\n&gt;&gt;&gt;  action = T.squeeze(action).item()\n&gt;&gt;&gt; print(f\"log prob {dist.log_prob(action)} squeezed = {T.squeeze(dist.log_prob(action))} Probs = {probs}\")\nlog prob tensor([-0.7533], device='cuda:0', grad_fn=&lt;SqueezeBackward1&gt;) squeezed = -0.753328263759613 Probs = -0.753328263759613\nThe code For Actor class. It derives from Torch.nn.Module\nclass AgentNetwork(nn.Module):\n    def __init__(self,input_dims,action_dim,lr,layer1=256,layer2=256,weight_file='weightFiles/ppo_discrete'):\n        super(AgentNetwork,self).__init__()\n        self.checkpoint_file = os.path.join(weight_file,'ppo_actor_weight')\n        #TOCHECK: *input_dims vs input_dims\n        self.actor = nn.Sequential(\n                nn.Linear(*input_dims,layer1),\n                nn.ReLU(),\n                nn.Linear(layer1,layer2),\n                nn.ReLU(),\n                nn.Linear(layer2,action_dim),\n                nn.Softmax(dim=-1)               \n        )\n        \n        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n        self.to(self.device)\n    \n    def forward(self,state):\n        dist = self.actor(state)\n        #TOCHECK: what does categorical do\n        dist = Categorical(dist)\n        return dist\n    \n    def save_checkpoint(self):\n        T.save(self.state_dict(),self.checkpoint_file)\n    \n    def load_checkpoint(self):\n        self.load_state_dict(T.load(self.checkpoint_file))\nCritic NN: takes in a state(tensor) and returns a tensor containing a single float corresponding to Value of that state\n&gt;&gt;&gt; state = T.tensor([observation],dtype=T.float).to(self.actor.device)\n&gt;&gt;&gt; value = self.critic(state)\n&gt;&gt;&gt; print(f\"value {value}\")\nvalue tensor([[-0.0977]], device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;)\nCode for Critic Class\n class CriticNetwork(nn.Module):\n    def __init__(self,input_dims,lr,layer1=256,layer2=256,weight_file='weightFiles/ppo_discrete'):\n        super(CriticNetwork,self).__init__()\n        self.checkpoint_file = os.path.join(weight_file,'ppo_critic_weight')\n        self.critic = nn.Sequential(\n                nn.Linear(*input_dims,layer1),\n                nn.ReLU(),\n                nn.Linear(layer1,layer2),\n                nn.ReLU(),\n                nn.Linear(layer2,1)\n        )\n        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n        self.to(self.device)\n        \n    def forward(self,state):\n        value = self.critic(state)\n        return value\n    \n    def save_checkpoint(self):\n        T.save(self.state_dict(),self.checkpoint_file)\n    \n    def load_checkpoint(self):\n        self.load_state_dict(T.load(self.checkpoint_file))"
  },
  {
    "objectID": "posts/PPO Implementation for Coders/index.html#learning-pseudo-code",
    "href": "posts/PPO Implementation for Coders/index.html#learning-pseudo-code",
    "title": "PPO Implementation for Coders",
    "section": "Learning Pseudo Code",
    "text": "Learning Pseudo Code\nPPO_agent_learn:\n  for _ in num_of_epoch:\n    compute advantage of the states in memory\n    generate batchs\n    Iterate over batches:\n      calculate cummulative actor loss for the batch\n      calculate cummulative critic loss for the batch\n      do gradient update\n  clear Memory"
  }
]