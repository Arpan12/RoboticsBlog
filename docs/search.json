[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RoboticsBlog",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 10, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nPPO Implementation for Coders\n\n\n\n\n\n\n\nRL\n\n\nCode\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nJun 10, 2023\n\n\nArpan Pallar\n\n\n\n\n\n\n  \n\n\n\n\nTensorFLow and Pytorch Cheat Sheet for RL\n\n\n\n\n\n\n\nRL\n\n\nCode\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nJun 10, 2023\n\n\nArpan Pallar\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/PPO Implementation for Coders/index.html",
    "href": "posts/PPO Implementation for Coders/index.html",
    "title": "PPO Implementation for Coders",
    "section": "",
    "text": "PPO is state of the art algorithm for DeepRL. We wont go in depth into theory but focus on implementation.\n\nPromise\nAfter reading till the end of the blog, you should be able to implement PPO for discrete action space based environments.\n\n\nTheory\nPPO is a policy gradient method\n\\(\\pi(u|x)\\to\\)our current policy. This in deep RL is usually approximate by a Neural Net called actor net\n\\(g(x_n,u_n)\\to\\) reward we get from the environment for taking action\n\\(V_\\pi(x) = g(x,\\pi(x)) + \\alpha*V_\\pi(x+1)\\) = value function for policy \\(\\pi\\) .\nif \\(J(\\theta) = V_\\pi(x_0)\\) approximate value function for an episode starting at x0 following policy \\(\\pi(\\theta)\\) ,We can decrease \\(J(\\theta)\\) by gradient descent \\(\\theta_{new} = \\theta - \\gamma*\\Delta J(\\theta)\\)\nso writing more mathematically\n\\(J(\\theta) = E_{u_n}\\sim \\pi_{\\theta}[\\sum_{n=0}^N\\alpha^n*g(x_n,u_n)]\\)\nwe are able to show(proof not important)\n\\(\\Delta_\\theta J(\\theta) = E[\\sum_{n=0}^N\\Psi_n\\Delta_nlog\\ \\pi(u_n|x_n,\\theta)]\\)\nwhich is similar to minimizing:\n\\(min_\\theta E[\\Psi_n*log\\ \\pi(u_n|x_n,|\\theta)]\\)\nThis is what we would use for actor loss and minimize\nfor\n\nReInforce: \\(\\Psi_n = G_n = \\sum_{k=n}^N \\alpha^k * g(x_k,u_k)\\)\nReInforce with baseLine: \\(\\Psi_n = G_n-V(x_n)\\)\nActor-Critic: \\(\\Psi_n = g(x_n,u_n)+\\alpha*V(x_{n+1})-V(x_n)\\)\nPPO: \\(\\Psi_n = A_n = Q(x_n,u_n)-V(x_n)\\)\nbut for PPO we approximate the log gradient even further by \\(min_\\theta \\sum[A_n\\frac{\\pi(u_n|x_n,Theta)}{\\pi(u_n| x_n,\\theta_{old})}]\\)\n\nFor PPO we generally clip the gradient by limiting between 1-\\(\\epsilon\\) and 1+\\(\\epsilon\\)\n\\(min_\\theta E[min(A_n\\ \\frac{\\pi(u_n|x_n,Theta)}{\\pi(u_n| x_n,\\theta_{old}},clip(\\frac{\\pi(u_n|x_n,Theta)}{\\pi(u_n| x_n,\\theta_{old}},1-\\epsilon,1+\\epsilon)A_n]\\)\nissue to address:\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html",
    "title": "TensorFLow and Pytorch Cheat Sheet for RL",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  }
]