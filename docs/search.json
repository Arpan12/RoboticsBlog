[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RoboticsBlog",
    "section": "",
    "text": "Non Linear Trajectory Optimization:Acrobat or Double Pendulum\n\n\n\n\n\n\n\ncasadi\n\n\nCode\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2023\n\n\nArpan Pallar\n\n\n\n\n\n\n  \n\n\n\n\nControlling NASA’s R2 Robot\n\n\n\n\n\n\n\nRobotics\n\n\nCode\n\n\nManipulators\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2023\n\n\nArpan Pallar\n\n\n\n\n\n\n  \n\n\n\n\nImplementing SLAM in ROS\n\n\n\n\n\n\n\nRobotics\n\n\nCode\n\n\nSLAM\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2023\n\n\nArpan Pallar\n\n\n\n\n\n\n  \n\n\n\n\nDDPG Implementation for Coders\n\n\n\n\n\n\n\nRL\n\n\nCode\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2023\n\n\nArpan Pallar\n\n\n\n\n\n\n  \n\n\n\n\nPPO Implementation for Coders\n\n\n\n\n\n\n\nRL\n\n\nCode\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nArpan Pallar\n\n\n\n\n\n\n  \n\n\n\n\nPython and Numpy cheat sheet for DL\n\n\n\n\n\n\n\nDL\n\n\nCode\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nJun 10, 2023\n\n\nArpan Pallar\n\n\n\n\n\n\n  \n\n\n\n\nPytorch and TensorFlowCheat Sheet for RL\n\n\n\n\n\n\n\nRL\n\n\nCode\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nJun 10, 2023\n\n\nArpan Pallar\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/PPO Implementation for Coders/index.html",
    "href": "posts/PPO Implementation for Coders/index.html",
    "title": "PPO Implementation for Coders",
    "section": "",
    "text": "PPO is state of the art algorithm for DeepRL. We ll see how this can be implemented in code(https://github.com/Arpan12/Reinforcement-Learning-Algos/blob/main/PPO_Discrete.ipynb)\nWe wont go in depth into theory but focus on implementation."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html",
    "title": "TensorFLow and Pytorch Cheat Sheet for RL",
    "section": "",
    "text": "Create a pytorch Neural Network"
  },
  {
    "objectID": "posts/Python,Numpy cheat sheet for DL/index.html",
    "href": "posts/Python,Numpy cheat sheet for DL/index.html",
    "title": "Python and Numpy cheat sheet for DL",
    "section": "",
    "text": "A cheat sheet for common numpy and Python APIs you would see and require for deep learning based programming for robotics"
  },
  {
    "objectID": "posts/Python,Numpy cheat sheet for DL/index.html#np.zeros",
    "href": "posts/Python,Numpy cheat sheet for DL/index.html#np.zeros",
    "title": "Python and Numpy cheat sheet for DL",
    "section": "np.zeros",
    "text": "np.zeros\nnumpy.zeros(shape, dtype=float, order=‘C’, *, like=None)\nReturn a new array of given shape and type, filled with zeros.\n\n\nParameters:\n\n\nshapeint or tuple of ints\n\nShape of the new array, e.g., (2, 3) or 2.\n\ndtypedata-type, optional\n\nThe desired data-type for the array, e.g., numpy.int8. Default is numpy.float64.\n\norder{‘C’, ‘F’}, optional, default: ‘C’\n\nWhether to store multi-dimensional data in row-major (C-style) or column-major (Fortran-style) order in memory.\n\nlikearray_like, optional\n\nReference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.\n\n\n\n\nnp.zeros(5)\narray([ 0.,  0.,  0.,  0.,  0.])  # default is numpy.float64\n&gt;&gt;&gt;np.zeros((5,), dtype=int) #if you want to create a 1D array\narray([0, 0, 0, 0, 0])\n\n&gt;&gt;&gt;np.zeros((5,1))\narray([[0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.]])\n       \n       \n&gt;&gt;&gt;s = (2,2)\n&gt;&gt;&gt;np.zeros(s)\narray([[ 0.,  0.],\n       [ 0.,  0.]]\n       \n#Multi-dim array \n&gt;&gt; np.zeros((4,3,2))  #a row of 2 element repeated 3 times-&gt; 3*2 array-&gt;repeated 4 times to give 4*3*2 array\n\narray([[[0., 0.],\n        [0., 0.],\n        [0., 0.]],\n\n       [[0., 0.],\n        [0., 0.],\n        [0., 0.]],\n\n       [[0., 0.],\n        [0., 0.],\n        [0., 0.]],\n\n       [[0., 0.],\n        [0., 0.],\n        [0., 0.]]])"
  },
  {
    "objectID": "posts/Python,Numpy cheat sheet for DL/index.html#get-and-set-numpy-array-value",
    "href": "posts/Python,Numpy cheat sheet for DL/index.html#get-and-set-numpy-array-value",
    "title": "Python and Numpy cheat sheet for DL",
    "section": "Get and Set Numpy Array Value",
    "text": "Get and Set Numpy Array Value\nx = np.zeros((4,3,2))\n#just like you would do in Python list or C array\nx[3][1][0]=1\nNegative Indexing"
  },
  {
    "objectID": "posts/Python,Numpy cheat sheet for DL/index.html#np.zeros_like",
    "href": "posts/Python,Numpy cheat sheet for DL/index.html#np.zeros_like",
    "title": "Python and Numpy cheat sheet for DL",
    "section": "np.zeros_like",
    "text": "np.zeros_like\n\nnumpy.zeros_like(a, dtype=None, order=‘K’, subok=True, shape=None)[source]\n\nReturn an array of zeros with the same shape and type as a given array.\n\nParameters:\n\n\naarray_like\n\nThe shape and data-type of a define these same attributes of the returned array.\n\ndtypedata-type, optional\n\nOverrides the data type of the result.\n\n\n\n\n\n\n&gt;&gt;&gt;x = np.arange(6)}\n&gt;&gt;&gt;x = x.reshape((2, 3))\n&gt;&gt;&gt;x\narray([[0, 1, 2],\n       [3, 4, 5]])\n&gt;&gt;&gt;np.zeros_like(x)\narray([[0, 0, 0],\n       [0, 0, 0]])"
  },
  {
    "objectID": "posts/Python,Numpy cheat sheet for DL/index.html#np.arange",
    "href": "posts/Python,Numpy cheat sheet for DL/index.html#np.arange",
    "title": "Python and Numpy cheat sheet for DL",
    "section": "np.arange",
    "text": "np.arange\n&gt;&gt;&gt;np.arange(6) # 1D array\narray([0, 1, 2, 3, 4, 5])"
  },
  {
    "objectID": "posts/Python,Numpy cheat sheet for DL/index.html#np.reshape",
    "href": "posts/Python,Numpy cheat sheet for DL/index.html#np.reshape",
    "title": "Python and Numpy cheat sheet for DL",
    "section": "np.reshape",
    "text": "np.reshape\n\nnumpy.reshape(a, newshape, order=‘C’)\n\nGives a new shape to an array without changing its data.\n\nParameters:\n\n\naarray_like\n\nArray to be reshaped.\n\nnewshapeint or tuple of ints\n\nThe new shape should be compatible with the original shape. If an integer, then the result will be a 1-D array of that length. One shape dimension can be -1. In this case, the value is inferred from the length of the array and remaining dimensions.\n\n\n\n\n&gt;&gt;&gt;a = np.arange(6).reshape((3, 2))}\n&gt;&gt;&gt;a       \n#notice it is row wise. It makes easier to visualize if you put all the element \n#in 1D row array. Then take start row wise rearrangement  \n\narray([[0, 1],\n       [2, 3],\n       [4, 5]])\n\n&gt;&gt;&gt;np.reshape(a, (2, 3)) # C-like index ordering\narray([[0, 1, 2],\n       [3, 4, 5]])\n       \n#there can be one -1 index meaning its dimention are infered from number of elements and remaining specified shape\n&gt;&gt;&gt;np.reshape(a, (3,-1))       # the unspecified value is inferred to be 2\narray([[1, 2],\n       [3, 4],\n       [5, 6]])"
  },
  {
    "objectID": "posts/Python,Numpy cheat sheet for DL/index.html#np.ones",
    "href": "posts/Python,Numpy cheat sheet for DL/index.html#np.ones",
    "title": "Python and Numpy cheat sheet for DL",
    "section": "np.ones:",
    "text": "np.ones:\neverything similar to np.zeros\nnp.ones((2, 1))\narray([[1.],\n       [1.]])"
  },
  {
    "objectID": "posts/Python,Numpy cheat sheet for DL/index.html#np-array-slices",
    "href": "posts/Python,Numpy cheat sheet for DL/index.html#np-array-slices",
    "title": "Python and Numpy cheat sheet for DL",
    "section": "np array slices",
    "text": "np array slices\nThe syntax of Python NumPy slicing is [start : stop : step]\n\nStart : This index by default considers as ‘0’\nstop : This index considers as a length of the array.\nstep : By default it is considered as ‘1’.\n\n#this is another way to create a numpy array\n&gt;&gt;&gt;arr = np.array([3, 5, 7, 9, 11, 15, 18, 22]) \n&gt;&gt;&gt; arr2 = arr[:5]\n# [ 3  5  7  9 11]\n&gt;&gt;&gt;arr2 = arr[-4:-2]\n# [11 15]\n&gt;&gt;&gt;arr2 = arr[::3]\n#[ 3  9 18]\n\n\narr = np.array([[3, 5, 7, 9, 11],\n               [2, 4, 6, 8, 10]])\n               \n\n# Use slicing a 2-D arrays\narr2 = arr[1:,1:3]  #2D array remains 2D array\n[[4 6]]\n\narr2 = arr[1,1:3] #2D array remains 1D array\n\n#now try solving this before looking at the solution\narr = np.array([[[3, 5, 7, 9, 11],\n                 [2, 4, 6, 8, 10]],\n                [[5, 7, 8, 9, 2],\n                 [7, 2, 3, 6, 7]]])\n               \narr2 = arr[0,1,0:2]\n[2 4]"
  },
  {
    "objectID": "posts/Python,Numpy cheat sheet for DL/index.html#np.eye",
    "href": "posts/Python,Numpy cheat sheet for DL/index.html#np.eye",
    "title": "Python and Numpy cheat sheet for DL",
    "section": "np.eye:",
    "text": "np.eye:\n\nnumpy.eye(N, M=None, k=0, dtype=&lt;class ‘float’&gt;, order=‘C’, *, like=None)\n\nReturn a 2-D array with ones on the diagonal and zeros elsewhere.\n\nParameters:\n\n\nNint\n\nNumber of rows in the output.\n\nMint, optional\n\nNumber of columns in the output. If None, defaults to N.\n\nkint, optional\n\nIndex of the diagonal: 0 (the default) refers to the main diagonal, a positive value refers to an upper diagonal, and a negative value to a lower diagonal.\n\ndtypedata-type, optional\n\nData-type of the returned array.\n\n\n\n\n\n\n&gt;&gt;&gt;np.eye(2, dtype=int)\narray([[1, 0],\n       [0, 1]])\n&gt;&gt;&gt; np.eye(3, k=1)\narray([[0.,  1.,  0.],\n       [0.,  0.,  1.],\n       [0.,  0.,  0.]])"
  },
  {
    "objectID": "posts/Python,Numpy cheat sheet for DL/index.html#reversedrangen",
    "href": "posts/Python,Numpy cheat sheet for DL/index.html#reversedrangen",
    "title": "Python and Numpy cheat sheet for DL",
    "section": "reversed(range(n))",
    "text": "reversed(range(n))\niterate in reverse ie. n-1,n-2,…0"
  },
  {
    "objectID": "posts/Python,Numpy cheat sheet for DL/index.html#np.random.shuffle",
    "href": "posts/Python,Numpy cheat sheet for DL/index.html#np.random.shuffle",
    "title": "Python and Numpy cheat sheet for DL",
    "section": "np.random.shuffle",
    "text": "np.random.shuffle\n\nrandom.shuffle(x)\n\nModify a sequence in-place by shuffling its contents.\nThis function only shuffles the array along the first axis of a multi-dimensional array. The order of sub-arrays is changed but their contents remains the same.\n\n\narr = np.arange(10)\n&gt;&gt;&gt; np.random.shuffle(arr)\n&gt;&gt;&gt; arr\n[1 7 5 2 9 4 3 6 0 8] # random\n\n# works for multi-D array also\n\narr = np.arange(9).reshape((3, 3))\nnp.random.shuffle(arr)\narr\narray([[3, 4, 5], # random\n       [6, 7, 8],\n       [0, 1, 2]])"
  },
  {
    "objectID": "posts/Python,Numpy cheat sheet for DL/index.html#numpy-axes",
    "href": "posts/Python,Numpy cheat sheet for DL/index.html#numpy-axes",
    "title": "Python and Numpy cheat sheet for DL",
    "section": "Numpy Axes",
    "text": "Numpy Axes\nfor a 2D array remember this figure\n\n&gt;&gt;&gt; np_array_2d = np.arange(0, 6).reshape([2,3])\n[[0 1 2]\n [3 4 5]]\n\n&gt;&gt;&gt;np.sum(np_array_2d, axis = 0) \narray([3, 5, 7])\n\n&gt;&gt;&gt;np.sum(np_array_2d, axis = 1) #2d array become 1D in either case \n\narray([3, 12])   # note its not array([[3],[12]])\n\n&gt;&gt;&gt; np_array_1s = np.array([[1,1,1],[1,1,1]])\narray([[1, 1, 1],\n       [1, 1, 1]])\n&gt;&gt;&gt; np_array_9s = np.array([[9,9,9],[9,9,9]])\narray([[9, 9, 9],\n       [9, 9, 9]])\n\n&gt;&gt;&gt; np.concatenate([np_array_1s, np_array_9s], axis = 0)\narray([[1, 1, 1],\n       [1, 1, 1],\n       [9, 9, 9],\n       [9, 9, 9]])\n\n&gt;&gt;&gt; np.concatenate([np_array_1s, np_array_9s], axis = 1)\narray([[1, 1, 1, 9, 9, 9],\n       [1, 1, 1, 9, 9, 9]])\n      \n# Be careful with 1D arrays are different. there is only 1 axis ie axis 0\n\n&gt;&gt;&gt; np_array_1s_1dim = np.array([1,1,1])\n&gt;&gt;&gt; np_array_9s_1dim = np.array([9,9,9])\n\n&gt;&gt;&gt; np.concatenate([np_array_1s_1dim, np_array_9s_1dim], axis = 0)\n\narray([1, 1, 1, 9, 9, 9])"
  },
  {
    "objectID": "posts/PPO Implementation for Coders/index.html#implementation",
    "href": "posts/PPO Implementation for Coders/index.html#implementation",
    "title": "PPO Implementation for Coders",
    "section": "Implementation",
    "text": "Implementation\n\nMake Actor and Critic NN\nactor NN: takes in a state(you need to make a tensor out of your state.Cant pass a list or np array) and spits out a Categorical object containing probs(because the last layer was softmax on action_dim)\nfor eg: in our case our observation is (4,) python list\n&gt;&gt;&gt; print(f\"observation= {observation}\")\nobservation= [ 0.02916372  0.02608052  0.01869606 -0.00384168]\nwe need to convert them to Tensor before passing them to NN\n&gt;&gt;&gt; state = T.tensor([observation],dtype=T.float).to(self.actor.device)\n&gt;&gt;&gt; print(f\"state= {state}\")\nstate= tensor([[ 0.0292,  0.0261,  0.0187, -0.0038]], device='cuda:0')\nwe get a tensor with Probability dist from NN(last layer was softmax). We then get an action by sampling the dist object\n&gt;&gt;&gt; dist = self.actor(state)\n&gt;&gt;&gt; print(f\"dist {dist} {dist.probs}\")\ndist Categorical(probs: torch.Size([1, 2])) tensor([[0.4913, 0.5087]], device='cuda:0', grad_fn=&lt;DivBackward0&gt;)\n\n&gt;&gt;&gt; action = dist.sample()\n&gt;&gt;&gt; print(f\"action = {action}\")\naction = tensor([1], device='cuda:0')\n\n# Retrieve action from action tensor and log_probs\n# squeeze removes all axises having value 1. item() returns a standard python float\n&gt;&gt;&gt;  probs = T.squeeze(dist.log_prob(action)).item()\n&gt;&gt;&gt;  action = T.squeeze(action).item()\n&gt;&gt;&gt; print(f\"log prob {dist.log_prob(action)} squeezed = {T.squeeze(dist.log_prob(action))} Probs = {probs}\")\nlog prob tensor([-0.7533], device='cuda:0', grad_fn=&lt;SqueezeBackward1&gt;) squeezed = -0.753328263759613 Probs = -0.753328263759613\nThe code For Actor class. It derives from Torch.nn.Module\nclass AgentNetwork(nn.Module):\n    def __init__(self,input_dims,action_dim,lr,layer1=256,layer2=256,weight_file='weightFiles/ppo_discrete'):\n        super(AgentNetwork,self).__init__()\n        self.checkpoint_file = os.path.join(weight_file,'ppo_actor_weight')\n        #TOCHECK: *input_dims vs input_dims\n        self.actor = nn.Sequential(\n                nn.Linear(*input_dims,layer1),\n                nn.ReLU(),\n                nn.Linear(layer1,layer2),\n                nn.ReLU(),\n                nn.Linear(layer2,action_dim),\n                nn.Softmax(dim=-1)               \n        )\n        \n        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n        self.to(self.device)\n    \n    def forward(self,state):\n        dist = self.actor(state)\n        #TOCHECK: what does categorical do\n        dist = Categorical(dist)\n        return dist\n    \n    def save_checkpoint(self):\n        T.save(self.state_dict(),self.checkpoint_file)\n    \n    def load_checkpoint(self):\n        self.load_state_dict(T.load(self.checkpoint_file))\nCritic NN: takes in a state(tensor) and returns a tensor containing a single float corresponding to Value of that state\n&gt;&gt;&gt; state = T.tensor([observation],dtype=T.float).to(self.actor.device)\n&gt;&gt;&gt; value = self.critic(state)\n&gt;&gt;&gt; print(f\"value {value}\")\nvalue tensor([[-0.0977]], device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;)\nCode for Critic Class\n class CriticNetwork(nn.Module):\n    def __init__(self,input_dims,lr,layer1=256,layer2=256,weight_file='weightFiles/ppo_discrete'):\n        super(CriticNetwork,self).__init__()\n        self.checkpoint_file = os.path.join(weight_file,'ppo_critic_weight')\n        self.critic = nn.Sequential(\n                nn.Linear(*input_dims,layer1),\n                nn.ReLU(),\n                nn.Linear(layer1,layer2),\n                nn.ReLU(),\n                nn.Linear(layer2,1)\n        )\n        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n        self.to(self.device)\n        \n    def forward(self,state):\n        value = self.critic(state)\n        return value\n    \n    def save_checkpoint(self):\n        T.save(self.state_dict(),self.checkpoint_file)\n    \n    def load_checkpoint(self):\n        self.load_state_dict(T.load(self.checkpoint_file))"
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#specify-optimizer-in-pytorch",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#specify-optimizer-in-pytorch",
    "title": "TensorFLow and Pytorch Cheat Sheet for RL",
    "section": "Specify Optimizer in Pytorch",
    "text": "Specify Optimizer in Pytorch"
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#theory-behind-assigning-tensor-to-device",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#theory-behind-assigning-tensor-to-device",
    "title": "TensorFLow and Pytorch Cheat Sheet for RL",
    "section": "Theory behind assigning tensor to device",
    "text": "Theory behind assigning tensor to device"
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#save-and-load-model-checkpoint-file",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#save-and-load-model-checkpoint-file",
    "title": "Pytorch and TensorFlowCheat Sheet for RL",
    "section": "Save and Load Model checkpoint File",
    "text": "Save and Load Model checkpoint File\nIf the NN class inherits from torch.nn.Module,then you can get the parameters/weights directly from self.state_dict() and save using torch.save()\nT.save(self.state_dict(),self.checkpoint_file)"
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#distribution",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#distribution",
    "title": "TensorFLow and Pytorch Cheat Sheet for RL",
    "section": "distribution",
    "text": "distribution"
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#t.squeeze",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#t.squeeze",
    "title": "Pytorch and TensorFlowCheat Sheet for RL",
    "section": "T.squeeze()",
    "text": "T.squeeze()\ntorch.squeeze(input, dim=None) → Tensor Returns a tensor with all specified dimensions of input of size 1 removed.\nFor example, if input is of shape: (A×1×B×C×1×D) then the input.squeeze() will be of shape: (A×B×C×D).\nWhen dim is given, a squeeze operation is done only in the given dimension(s). If input is of shape: (A×1×B), squeeze(input, 0) leaves the tensor unchanged, but squeeze(input, 1) will squeeze the tensor to the shape (A×B). Parameters: input (Tensor) – the input tensor.\ndim (int or tuple of ints, optional) –if given, the input will be squeezed only in the specified dimensions\n&gt;&gt;&gt; x = torch.zeros(2, 1, 2, 1, 2) \n&gt;&gt;&gt; x.size() \ntorch.Size([2, 1, 2, 1, 2]) \n&gt;&gt;&gt; y = torch.squeeze(x) \n&gt;&gt;&gt; y.size() \ntorch.Size([2, 2, 2]) \n&gt;&gt;&gt; y = torch.squeeze(x, 0) \n&gt;&gt;&gt; y.size() \ntorch.Size([2, 1, 2, 1, 2]) \n&gt;&gt;&gt; y = torch.squeeze(x, 1) \n&gt;&gt;&gt; y.size() \ntorch.Size([2, 2, 1, 2]) \n&gt;&gt;&gt; y = torch.squeeze(x, (1, 2, 3)) \ntorch.Size([2, 2, 2])"
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#numpy-to-tensor",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#numpy-to-tensor",
    "title": "Pytorch and TensorFlowCheat Sheet for RL",
    "section": "numpy to Tensor",
    "text": "numpy to Tensor\n&gt;&gt;&gt; print(f\"observation= {observation}\") \nobservation= [ 0.02916372 0.02608052 0.01869606 -0.00384168]\n&gt;&gt;&gt; state = T.tensor([observation],dtype=T.float).to(self.actor.device)\n&gt;&gt;&gt; print(f\"state= {state}\")\nstate= tensor([[ 0.0292,  0.0261,  0.0187, -0.0038]], device='cuda:0')\n\n\n# Another way. convert from numpy to PyTorch\nnp_array = np.ones((2,3))\npytorch_tensor = torch.from_numpy(np_array)\n# convert from PyTorch to numpy\npt_tensor = torch.rand(2,3)\nnumpy_array = pt_tensor.numpy()\n# they share same underlying memory. So, changes to one shall be reflected on the other. \n# for ex : pt_tensor update results in an update to numpy_array."
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#optimizer.zero_grad",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#optimizer.zero_grad",
    "title": "Pytorch and TensorFlowCheat Sheet for RL",
    "section": "optimizer.zero_grad",
    "text": "optimizer.zero_grad"
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#total_loss.backward",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#total_loss.backward",
    "title": "Pytorch and TensorFlowCheat Sheet for RL",
    "section": "total_loss.backward",
    "text": "total_loss.backward"
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#optimizer.step",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#optimizer.step",
    "title": "Pytorch and TensorFlowCheat Sheet for RL",
    "section": "optimizer.step",
    "text": "optimizer.step"
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#pytorch-softmax-distribution",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#pytorch-softmax-distribution",
    "title": "Pytorch and TensorFlowCheat Sheet for RL",
    "section": "PyTorch SoftMax / Distribution",
    "text": "PyTorch SoftMax / Distribution\nwe get a tensor with Probability dist from NN(last layer was softmax). We then get an action by sampling the dist object\n&gt;&gt;&gt; dist = self.actor(state)\n&gt;&gt;&gt; print(f\"dist {dist} {dist.probs}\")\ndist Categorical(probs: torch.Size([1, 2])) tensor([[0.4913, 0.5087]], device='cuda:0', grad_fn=&lt;DivBackward0&gt;)\n\n&gt;&gt;&gt; action = dist.sample()\n&gt;&gt;&gt; print(f\"action = {action}\")\naction = tensor([1], device='cuda:0')"
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#optimizer-in-pytorch",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#optimizer-in-pytorch",
    "title": "TensorFLow and Pytorch Cheat Sheet for RL",
    "section": "Optimizer in Pytorch",
    "text": "Optimizer in Pytorch"
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#torch.tensor",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#torch.tensor",
    "title": "Pytorch and TensorFlowCheat Sheet for RL",
    "section": "Torch.Tensor",
    "text": "Torch.Tensor\nPyTorch provides torch.Tensor to represent a multi-dimensional array containing elements of a single data type\nIt is basically the same as a numpy array: it does not know anything about deep learning or computational graphs or gradients, and is just a generic n-dimensional array to be used for arbitrary numeric computation. The biggest difference between a numpy array and a PyTorch Tensor is that a PyTorch Tensor can run on either CPU or GPU. To run operations on the GPU, just cast the Tensor to a cuda datatype.\nThere are three attributes : torch.dtype, torch.device and torch.layout\na = torch.rand(2,2, device='cpu')\n# transfer tensor created on cpu to gpu accessible memory.\nif torch.cuda.is_available():\n      device = torch.device('cuda') # create a device handle\n       a = a.to(device) # pass device handle created."
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#copying-tensor",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#copying-tensor",
    "title": "Pytorch and TensorFlowCheat Sheet for RL",
    "section": "Copying Tensor",
    "text": "Copying Tensor"
  },
  {
    "objectID": "posts/PPO Implementation for Coders/index.html#batches-and-how-to-generate-them",
    "href": "posts/PPO Implementation for Coders/index.html#batches-and-how-to-generate-them",
    "title": "PPO Implementation for Coders",
    "section": "Batches and How to generate them",
    "text": "Batches and How to generate them\nWe need to take all the elements in the memory and create batches out of them.\nGOAL\n1.The elements should not repeat from one batch to another\n2.There should be any correlation between elements of same batch.\nHere is how we achieve that in code\n  def generate_batches(self):\n        n_states = len(self.states)\n        batches=[]\n        i=0\n        indices = np.arange(n_states,dtype = np.int64)\n        np.random.shuffle(indices)\n        for i in range(n_states):\n            batches.append(indices[i:i+self.batch_size])\n            i+=self.batch_size\n        return np.array(self.states),\\\n                np.array(self.actions),\\\n                np.array(self.probs), \\\n                np.array(self.vals),\\\n                np.array(self.rewards), \\\n                np.array(self.dones),\\\n                batches\n#Note here only batch index get suffled. We need to have the actual order of states,actions,probs,vals,rewards and done so that we can calculate advatage of each state later"
  },
  {
    "objectID": "posts/PPO Implementation for Coders/index.html#compute-the-advantage",
    "href": "posts/PPO Implementation for Coders/index.html#compute-the-advantage",
    "title": "PPO Implementation for Coders",
    "section": "compute the advantage",
    "text": "compute the advantage\nWe know that advantage of a state is given by following equation\n\\(A_t = \\sum_{t=k}^N (\\lambda *\\mu)^{t-k}*( Q(x_n,u_n)-V(x_n))\\)\nHere is how we achieve that in code\nstate_arr,action_arr,probs_arr,vals_arr, \\\n    rewards_arr,dones_arr,batches = self.memory.generate_batches()\n            \nadvantages=np.zeros_like(rewards_arr)\n            \nfor t in reversed(range(len(state_arr)-1)):\n    advantages[t] = rewards_arr[t]+self.gamma*vals_arr[t+1]*(1-int(dones_arr[t]))-vals_arr[t] + self.gamma*self.lambda_factor*advantages[t+1]\n                \nadvantages = T.tensor(advantages).to(self.actor.device)"
  },
  {
    "objectID": "posts/PPO Implementation for Coders/index.html#loss-function",
    "href": "posts/PPO Implementation for Coders/index.html#loss-function",
    "title": "PPO Implementation for Coders",
    "section": "loss function",
    "text": "loss function\nfor actor loss,we know that we need to minimize\n\\(min_\\theta E[\\sum_{n=0}^N min(A_n\\ \\frac{\\pi(u_n|x_n,Theta)}{\\pi(u_n| x_n,\\theta_{old}},clip(\\frac{\\pi(u_n|x_n,Theta)}{\\pi(u_n| x_n,\\theta_{old}},1-\\epsilon,1+\\epsilon)A_n)]\\)\nso we can find gradient of this function with respect to our weights \\(\\Theta\\) and do a 1 step gradient descent\nThis can be done in Code by\n states = T.tensor(state_arr[batch],dtype = T.float).to(self.actor.device)\nactions = T.tensor(action_arr[batch],dtype = T.float).to(self.actor.device)\nold_probs = T.tensor(probs_arr[batch],dtype = T.float).to(self.actor.device)\ndist = self.actor(states)\nnew_probs = dist.log_prob(actions)\n#TOCHECK: what do exp() do\n                \nprob_ratio = new_probs.exp()/old_probs.exp()\n                \nweighted_prob = advantages[batch]*(prob_ratio)\n                \nweighted_clipped_probs = T.clamp(prob_ratio,1-self.policy_clip,1+self.policy_clip)*advantages[batch]\n                \nactor_loss = - T.min(weighted_clipped_probs,weighted_prob).mean()\nfor Critic loss,we know that we need to minimize the error of values of our state \\((V_{des} - V_{pred})^2\\)\nwhere\n\\(V_{des} = a_t+value_{theta\\_old}\\)\n\\(V_{pred} = value \\ from \\ critic \\ NN\\)\nThis is done in code as\ncritic_values = T.squeeze(critic_values)\n                \ndesired_state_values = advantages[batch]+values[batch]\ncritic_loss = (desired_state_values-critic_values)**2\ncritic_loss = critic_loss.mean()\nThen We calculate the total loss and do 1 step gradient descent as\n total_loss = actor_loss+critic_loss*0.5\n                \nself.actor.optimizer.zero_grad()\nself.critic.optimizer.zero_grad()\ntotal_loss.backward()\nself.actor.optimizer.step()\nself.critic.optimizer.step()"
  },
  {
    "objectID": "posts/PPO Implementation for Coders/index.html#make-actor-and-critic-nn",
    "href": "posts/PPO Implementation for Coders/index.html#make-actor-and-critic-nn",
    "title": "PPO Implementation for Coders",
    "section": "Make Actor and Critic NN",
    "text": "Make Actor and Critic NN\nactor NN: takes in a state(you need to make a tensor out of your state.Cant pass a list or np array) and spits out a Categorical object containing probs(because the last layer was softmax on action_dim)\nfor eg: in our case our observation is (4,) python list\n&gt;&gt;&gt; print(f\"observation= {observation}\")\nobservation= [ 0.02916372  0.02608052  0.01869606 -0.00384168]\nwe need to convert them to Tensor before passing them to NN\n&gt;&gt;&gt; state = T.tensor([observation],dtype=T.float).to(self.actor.device)\n&gt;&gt;&gt; print(f\"state= {state}\")\nstate= tensor([[ 0.0292,  0.0261,  0.0187, -0.0038]], device='cuda:0')\nwe get a tensor with Probability dist from NN(last layer was softmax). We then get an action by sampling the dist object\n&gt;&gt;&gt; dist = self.actor(state)\n&gt;&gt;&gt; print(f\"dist {dist} {dist.probs}\")\ndist Categorical(probs: torch.Size([1, 2])) tensor([[0.4913, 0.5087]], device='cuda:0', grad_fn=&lt;DivBackward0&gt;)\n\n&gt;&gt;&gt; action = dist.sample()\n&gt;&gt;&gt; print(f\"action = {action}\")\naction = tensor([1], device='cuda:0')\n\n# Retrieve action from action tensor and log_probs\n# squeeze removes all axises having value 1. item() returns a standard python float\n&gt;&gt;&gt;  probs = T.squeeze(dist.log_prob(action)).item()\n&gt;&gt;&gt;  action = T.squeeze(action).item()\n&gt;&gt;&gt; print(f\"log prob {dist.log_prob(action)} squeezed = {T.squeeze(dist.log_prob(action))} Probs = {probs}\")\nlog prob tensor([-0.7533], device='cuda:0', grad_fn=&lt;SqueezeBackward1&gt;) squeezed = -0.753328263759613 Probs = -0.753328263759613\nThe code For Actor class. It derives from Torch.nn.Module\nclass AgentNetwork(nn.Module):\n    def __init__(self,input_dims,action_dim,lr,layer1=256,layer2=256,weight_file='weightFiles/ppo_discrete'):\n        super(AgentNetwork,self).__init__()\n        self.checkpoint_file = os.path.join(weight_file,'ppo_actor_weight')\n        #TOCHECK: *input_dims vs input_dims\n        self.actor = nn.Sequential(\n                nn.Linear(*input_dims,layer1),\n                nn.ReLU(),\n                nn.Linear(layer1,layer2),\n                nn.ReLU(),\n                nn.Linear(layer2,action_dim),\n                nn.Softmax(dim=-1)               \n        )\n        \n        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n        self.to(self.device)\n    \n    def forward(self,state):\n        dist = self.actor(state)\n        #TOCHECK: what does categorical do\n        dist = Categorical(dist)\n        return dist\n    \n    def save_checkpoint(self):\n        T.save(self.state_dict(),self.checkpoint_file)\n    \n    def load_checkpoint(self):\n        self.load_state_dict(T.load(self.checkpoint_file))\nCritic NN: takes in a state(tensor) and returns a tensor containing a single float corresponding to Value of that state\n&gt;&gt;&gt; state = T.tensor([observation],dtype=T.float).to(self.actor.device)\n&gt;&gt;&gt; value = self.critic(state)\n&gt;&gt;&gt; print(f\"value {value}\")\nvalue tensor([[-0.0977]], device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;)\nCode for Critic Class\n class CriticNetwork(nn.Module):\n    def __init__(self,input_dims,lr,layer1=256,layer2=256,weight_file='weightFiles/ppo_discrete'):\n        super(CriticNetwork,self).__init__()\n        self.checkpoint_file = os.path.join(weight_file,'ppo_critic_weight')\n        self.critic = nn.Sequential(\n                nn.Linear(*input_dims,layer1),\n                nn.ReLU(),\n                nn.Linear(layer1,layer2),\n                nn.ReLU(),\n                nn.Linear(layer2,1)\n        )\n        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n        self.to(self.device)\n        \n    def forward(self,state):\n        value = self.critic(state)\n        return value\n    \n    def save_checkpoint(self):\n        T.save(self.state_dict(),self.checkpoint_file)\n    \n    def load_checkpoint(self):\n        self.load_state_dict(T.load(self.checkpoint_file))"
  },
  {
    "objectID": "posts/PPO Implementation for Coders/index.html#learning-pseudo-code",
    "href": "posts/PPO Implementation for Coders/index.html#learning-pseudo-code",
    "title": "PPO Implementation for Coders",
    "section": "Learning Pseudo Code",
    "text": "Learning Pseudo Code\nPPO_agent_learn:\n  for _ in num_of_epoch:\n    compute advantage of the states in memory\n    generate batchs\n    Iterate over batches:\n      calculate cummulative actor loss for the batch\n      calculate cummulative critic loss for the batch\n      do gradient update\n  clear Memory"
  },
  {
    "objectID": "posts/DDPG Implementation for Coders/index.html",
    "href": "posts/DDPG Implementation for Coders/index.html",
    "title": "DDPG Implementation for Coders",
    "section": "",
    "text": "DDPG is a deepRL algorithm for agents with continuous actions space and Useful Particularly in the field of robotics where actuators and motors take continuous signal . We ll see how this can be implemented in code (https://github.com/Arpan12/Reinforcement-Learning-Algos/blob/main/DDPG.ipyn)\nWe wont go in depth into theory but focus on implementation."
  },
  {
    "objectID": "posts/DDPG Implementation for Coders/index.html#make-actor-and-critic-nn",
    "href": "posts/DDPG Implementation for Coders/index.html#make-actor-and-critic-nn",
    "title": "DDPG Implementation for Coders",
    "section": "Make Actor and Critic NN",
    "text": "Make Actor and Critic NN\nWe need a critic NN that takes in state and action value and output a single float i.e. Q value of the state. Also, we need a Actor NN that takes in the state and tried to predict the action values. If a robot has 3 joints , the actor NN would output 3 values between 1 and -1\n#We use keras to form our model.\n#note critic returns just one value. Actor return N=action dim values between 1 to -1 \n#we do not need to specify the inout dimensions in the init. It picks that up from call function\n\nimport tensorflow.keras as keras\nfrom tensorflow.keras.layers import Dense\n\n\nclass CriticNetwork(keras.Model):\n  def __init__(self,layer1_dim=512,layer2_dim=512,chkpt_dir='weightFiles    /DDPG',name = \"Critic\"):\n        super(CriticNetwork,self).__init__()\n        self.layer1 = Dense(layer1_dim,activation = 'relu')\n        self.layer2 = Dense(layer2_dim,activation='relu')\n        self.v = Dense(1,activation=None)\n        self.checkpoint_file = os.path.join(chkpt_dir,name+\"_DDPG.h5\")\n        \n    def call(self,state,action):\n        x = self.layer1(tf.concat([state,action],axis=1))\n        x = self.layer2(x)\n        q = self.v(x)\n        return q\n    \n\nclass ActorNetwork(keras.Model):\n    def __init__(self,layer1_dim=512,layer2_dim=512,n_action_dim = 2,chkpt_dir=\"weightFiles/DDPG\",name = \"Actor\"):\n        super(ActorNetwork,self).__init__()\n        self.layer1 =  Dense(layer1_dim,activation='relu')\n        self.layer2 = Dense(layer2_dim,activation='relu')\n        self.actions = Dense(n_action_dim,activation='tanh')\n        self.checkpoint_file = os.path.join(chkpt_dir,name+\"_DDPG.h5\")\n    \n    def call(self,state):\n        x = self.layer1(state)\n        x = self.layer2(x)\n        actions = self.actions(x)\n        return actions"
  },
  {
    "objectID": "posts/DDPG Implementation for Coders/index.html#batches-and-how-to-generate-them",
    "href": "posts/DDPG Implementation for Coders/index.html#batches-and-how-to-generate-them",
    "title": "DDPG Implementation for Coders",
    "section": "Batches and How to generate them",
    "text": "Batches and How to generate them\nWe create a replay memory where we store some large number of of (state,next_state,action_memory,reward_memory,done) transitions(in our case max is 1000000).\nclass ReplayBuffer:\n    def __init__(self,max_size,state_shape,action_shape):\n        self.mem_cnt=0\n        self.max_size = max_size\n        self.states = np.zeros((max_size,*state_shape))\n        self.next_states = np.zeros((max_size,*state_shape))\n        self.actions_memory = np.zeros((max_size,*action_shape))\n        self.rewards_memory = np.zeros((max_size,))\n        self.dones = np.zeros((max_size,))\n    \n    def store_transition(self,state,next_state,action,reward,done):\n        self.mem_cnt+=1\n        index = self.mem_cnt%self.max_size\n        self.states[index] = state\n        self.next_states[index] = next_state\n        self.actions_memory[index] = action\n        self.rewards_memory[index] = reward\n        self.dones[index] = done\nFor training we sample this memory for a batch of given size (in our case 64) and try to minimize our loss on that.\n    def sample_buffer(self,batch_size):\n        max_mem = min(self.mem_cnt,self.max_size)\n        batch = np.random.choice(max_mem,batch_size,replace = False)\n        \n        states = self.states[batch]\n        next_states = self.next_states[batch]\n        actions = self.actions_memory[batch]\n        rewards = self.rewards_memory[batch]\n        dones = self.dones[batch]\n        return states,next_states,actions,rewards,dones"
  },
  {
    "objectID": "posts/DDPG Implementation for Coders/index.html#compute-the-advantage",
    "href": "posts/DDPG Implementation for Coders/index.html#compute-the-advantage",
    "title": "DDPG Implementation for Coders",
    "section": "compute the advantage",
    "text": "compute the advantage\nWe know that advantage of a state is given by following equation\n\\(A_t = \\sum_{t=k}^N (\\lambda *\\mu)^{t-k}*( Q(x_n,u_n)-V(x_n))\\)\nHere is how we achieve that in code\nstate_arr,action_arr,probs_arr,vals_arr, \\\n    rewards_arr,dones_arr,batches = self.memory.generate_batches()\n            \nadvantages=np.zeros_like(rewards_arr)\n            \nfor t in reversed(range(len(state_arr)-1)):\n    advantages[t] = rewards_arr[t]+self.gamma*vals_arr[t+1]*(1-int(dones_arr[t]))-vals_arr[t] + self.gamma*self.lambda_factor*advantages[t+1]\n                \nadvantages = T.tensor(advantages).to(self.actor.device)"
  },
  {
    "objectID": "posts/DDPG Implementation for Coders/index.html#loss-function",
    "href": "posts/DDPG Implementation for Coders/index.html#loss-function",
    "title": "DDPG Implementation for Coders",
    "section": "Loss function",
    "text": "Loss function\nFor critic loss we take sum of \\(\\Delta t_d^2\\) where \\(t_d = g(x_n,u_n)+\\gamma*V(x_{n+1)}-V(x_n)\\) for the entire batch and try to minimize it.\nFirst you need to sample a batch and covert everything to tensor so that your NN can do the forward and back prop on it.\n        states,next_states,actions,rewards,dones = self.memory.sample_buffer(self.batch_size)\n        states = tf.convert_to_tensor(states,dtype=tf.float32)\n        next_states = tf.convert_to_tensor(next_states,dtype=tf.float32)\n        actions = tf.convert_to_tensor(actions,dtype=tf.float32)\n        rewards = tf.convert_to_tensor(rewards,dtype=tf.float32)\n        dones = tf.convert_to_tensor(dones,dtype=tf.float32)\n        \nAfter you have the batch then you can find the critic loss = sum of td error. But remember you need gradient of critic loss so put it inside GradientTape so Pytorch knows that it needs to form a computation graph.\n        with tf.GradientTape() as tape:\n            target_actions = self.target_actor(next_states)\n            \n            next_critic_val = tf.squeeze(self.target_critic(next_states,target_actions),1)\n            \n            critic_val = tf.squeeze(self.critic(states,actions),1)\n            target = rewards+self.gamma*next_critic_val*(1-done)\n            critic_loss = keras.losses.MSE(target,critic_val)\n\n#calculate the gradient and apply it\n        critic_network_gradient = tape.gradient(critic_loss,self.critic.trainable_variables) \n        self.critic.optimizer.apply_gradients(zip(critic_network_gradient,self.critic.trainable_variables))\nFor Actor loss,as explained earlier, we take loss function as L = -V(state) = - critic(state,actor(state))\n        with tf.GradientTape() as tape:\n            new_policy_actions = self.actor(states)\n            actor_loss = -self.critic(states,new_policy_actions)\n            actor_loss = tf.math.reduce_mean(actor_loss)\n        \n        actor_network_gradient = tape.gradient(actor_loss,self.actor.trainable_variables)\n        \n        self.actor.optimizer.apply_gradients(zip(actor_network_gradient,self.actor.trainable_variables))"
  },
  {
    "objectID": "posts/DDPG Implementation for Coders/index.html#learning-pseudo-code",
    "href": "posts/DDPG Implementation for Coders/index.html#learning-pseudo-code",
    "title": "DDPG Implementation for Coders",
    "section": "Learning Pseudo Code",
    "text": "Learning Pseudo Code\nPPO_agent_learn:\n  for _ in num_of_epoch:\n    compute advantage of the states in memory\n    generate batchs\n    Iterate over batches:\n      calculate cummulative actor loss for the batch\n      calculate cummulative critic loss for the batch\n      do gradient update\n  clear Memory"
  },
  {
    "objectID": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#create-a-tensorflow-neural-network",
    "href": "posts/TensorFlow and Pytorch Cheetsheet for RL/index.html#create-a-tensorflow-neural-network",
    "title": "Pytorch and TensorFlowCheat Sheet for RL",
    "section": "Create a TensorFlow Neural Network",
    "text": "Create a TensorFlow Neural Network\nSet learning Rate\nIterate over weights\nsave and load weights\nnumpy to tensor\ntf.random.normal\ntf.clip by value\ntf.gradient_tape\ntape.gradient\napply_gradient\nkeras.losses.MSE"
  },
  {
    "objectID": "posts/Python,Numpy cheat sheet for DL/index.html#np-random-choice",
    "href": "posts/Python,Numpy cheat sheet for DL/index.html#np-random-choice",
    "title": "Python and Numpy cheat sheet for DL",
    "section": "Np random choice",
    "text": "Np random choice"
  },
  {
    "objectID": "posts/SLAM in ROS/index.html",
    "href": "posts/SLAM in ROS/index.html",
    "title": "Implementing SLAM in ROS",
    "section": "",
    "text": "Here is a project on How to implement slam in ROS. This project explains how to interface with various sensors and actuators as well as how to develop your project step by step in the simulator called Gazebo. We assume basic familiarity with ROS.This blog is heavily influenced from Programming Robots with ROS book from Morgan Quigley which I highly recommend."
  },
  {
    "objectID": "posts/SLAM in ROS/index.html#slam-in-ros",
    "href": "posts/SLAM in ROS/index.html#slam-in-ros",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Here is a project on How to implement slam in ROS. This project explains how to interface with various sensors and actuators as well as how to develop your project step by step in the simulator called Gazebo."
  },
  {
    "objectID": "posts/SLAM in ROS/index.html#step-1",
    "href": "posts/SLAM in ROS/index.html#step-1",
    "title": "Implementing SLAM in ROS",
    "section": "step 1:",
    "text": "step 1:\ncreate a wander bot that scans any obstacles around it and avoids it\nstep 1.1: mkdir -p ~/wanderbot_ws/src\nstep 1.2: cd ~wanderbot_ws/src\nstep 1.3: catkin_init_workspace\nstep 1.4: cd ~/wanderbot_ws/src\nstep 1.5: catkin_create_pkg wanderbot rospy geometry_msgs sensor_msgs\nthis would create a package in your catkin_ws called wanderbot. Inside the package notice two files\n1.CMakeLists.txt : a starting point for the build script for this package\n2.package.xml: a machine-readable description of the package, including details such as its name, description, author, license, and which other packages it depends on to build and run\nNow inside the src of the package wanderbot write a python program that\n\nhas a publisher node to publish the cmd velocity to the wanderbot. The cmd velocity would be of type Twist\n from geometry_msgs.msg import Twist\n cmd_vel_pub = rospy.Publisher('cmd_vel', Twist, queue_size=1)\nHere we publish to the topic cmd_vel messages of type Twist and queue size=1 tells rospy to buffer 1 outgoing msg. In case the node sending the messages is transmitting at a higher rate than the receiving node(s) can receive them, rospy will simply drop any messages beyond the queue_size\npublish message into this topic cmd_vel at 10 hz per second so as to not to send too many msgs\nrate = rospy.Rate(10)\nwhile not rospy.is_shutdown():\n  .....\n\n  rate.sleep()\nMessages of type Twist can be published that it then received by a subscriber node that run on the robot.\nmsg definition of Twist:\n# This expresses velocity in free space broken into its linear and angular parts.\nVector3 linear\nVector3 angular  \nso you can set a forward velocity of 0.5 as follows:\n\ngreen_light_twist = Twist()\ngreen_light_twist.linear.x = 0.5\n\nTo scan for any obstacles we use the laser can on boarded on the turtle-bot robot. It gives a linear vector of ranges from the robot to the nearest obstacles in various directions\n# Single scan from a planar laser range-finder\nHeader header            # timestamp in the header is the acquisition time of \n                         # the first ray in the scan.\n                         #\n                         # in frame frame_id, angles are measured around \n                         # the positive Z axis (counterclockwise, if Z is up)\n                         # with zero angle being forward along the x axis\n\nfloat32 angle_min        # start angle of the scan [rad]\nfloat32 angle_max        # end angle of the scan [rad]\nfloat32 angle_increment  # angular distance between measurements [rad]\n\nfloat32 time_increment   # time between measurements [seconds] - if your scanner\n                         # is moving, this will be used in interpolating position\n                         # of 3d points\nfloat32 scan_time        # time between scans [seconds]\n\nfloat32 range_min        # minimum range value [m]\nfloat32 range_max        # maximum range value [m]\n\nfloat32[] ranges         # range data [m] (Note: values &lt; range_min or &gt; range_max should be discarded)\nfloat32[] intensities    # intensity data [device-specific units].  If your\n                         # device does not provide intensities, please leave\n                         # the array empty.\n\nWe can calculate the bearing as:\nbearing = msg.angle_min + i * msg.angle_max / len(msg.ranges)\nWe can calculate the nearest obstacle directly in front of the robot by fetching the middle elements of the ranges\nrange_ahead = msg.ranges[len(msg.ranges)/2]\n\nWrite a subscriber to the Laser Scan data we receive from turtle bot\n\nfrom sensor_msgs.msg import LaserScan\ndef scan_callback(msg):\n  range_ahead = msg.ranges[len(msg.ranges)/2]\n  print \"range ahead: %0.1f\" % range_ahead\n\nscan_sub = rospy.Subscriber('scan', LaserScan, scan_callback)\n\nnext we can write a obstacle avoidance logic on basis of following psedo-code:\nif range_ahead &lt; 0.8:\n    publisher send a twist of velocity 0 and non-zero angular velocity \nelse:\n     publisher send a twist of velocity non zeros and zero angular velocity \nYou can run it like\nroslaunch turtlebot_gazebo turtlebot_world.launch\n\nchmod +x &lt;your pythonfile&gt;.py\n/&lt;your pythonfile&gt;.py cmd_vel:=cmd_vel_mux/input/teleop"
  },
  {
    "objectID": "posts/SLAM in ROS/index.html#keyboard-to-move-your-robot",
    "href": "posts/SLAM in ROS/index.html#keyboard-to-move-your-robot",
    "title": "Implementing SLAM in ROS",
    "section": "Keyboard to Move your Robot",
    "text": "Keyboard to Move your Robot\n\nCreate a node that read the keyboard input.Make a publisher that forwards the read keyboard into a topic called keys.\nHow to do it in code\nimport sys,select,tty,termios\n\n#get current attribute/setting of terminal\nold_attr = termios.tcgetattr(sys.stdin)\n\n#set the setting so as to read every character and not wait till full line\ntty.setcbreak(sys.stdin.fileno())\n\nkey_pub = rospy.Publisher('keys',String,queue_size=1)\n\nif select.select([sys.stdin],[],[],0)[0]==[sys.stdin]:\n        key_pub.publish(sys.stdin.read(1))\n\n#reset back to the original setting\ntermios.tcsetattr(sys.stdin,termios.TCSADRAIN,old_attr)\nAnother node that subscribe to the keys and publishes a twist\n\nkey_mapping = {'w':[0,1],'x':[0,-1],\n               'a':[-1,0],'d':[1,0],\n               's':[0,0]}\n\ng_last_twist = None\ndef keys_cb(msg,twist_pub):\n    global g_last_twist\n    if len(msg.data) == 0 or not key_mapping.has_key(msg.data[0]):\n        return\n    \n    vels = key_mapping[msg.data[0]]\n    g_last_twist.angular.z = vels[0]\n    g_last_twist.linear.x = vels[1]\n    \nrospy.Subscriber('keys',String,keys_cb,twist_pub)\n\n# define publisher\ntwist_pub = rospy.Publisher('cmd_vel',Twist,queue_size=1)\ntwist_pub.publish(g_last_twist)\n\nDebug TIPS\n\nTo discover topic data : rostopic info cmd_vel\nThis will print information about the topic publishers and subscribers,as well as stating that the cmd_vel topic is of type geometry_msgs/Twist .\nrosmsg show geometry_msgs/Twist\n&gt;&gt;&gt; geometry_msgs/Vector3 linear\nfloat64 x\nfloat64 y\nfloat64 z\ngeometry_msgs/Vector3 angular\nfloat64 x\nfloat64 y\nfloat64 z\nTo plot in real time : rqt_plot cmd_vel/linear/x cmd_vel/angular/z"
  },
  {
    "objectID": "posts/DDPG Implementation for Coders/index.html#target-and-non-target-nn.-when-is-each-used-in-training",
    "href": "posts/DDPG Implementation for Coders/index.html#target-and-non-target-nn.-when-is-each-used-in-training",
    "title": "DDPG Implementation for Coders",
    "section": "Target and Non-Target NN. when is each used in training",
    "text": "Target and Non-Target NN. when is each used in training\n\nTarget weight trails behind non target weight in update. The learning ie. Gradient descent is done on non target weight for a batch and then target is updated like target_weight = 0.995*target_weight+0.005*non_target weight\nBut when calculating critic and actor loss , while we want Q val of a state action pair, Target network is taken. The rational behind this is that target is slow moving hence more stable to current sampling that might move too randomly to converge"
  },
  {
    "objectID": "posts/SLAM in ROS/index.html#parameter-server-in-ros",
    "href": "posts/SLAM in ROS/index.html#parameter-server-in-ros",
    "title": "Implementing SLAM in ROS",
    "section": "Parameter Server in ROS",
    "text": "Parameter Server in ROS\na parameter server is a generic key/value store. We can pass parameter from cmdline while starting the program\n./keys_to_twist_parameterized.py _linear_scale:=0.5 _angular_scale:=0.4\nand then you can use fetch the parameters from inside the program using\nif rospy.has_param('~linear_scale'):\n  g_vel_scales[1] = rospy.get_param('~linear_scale')\nIt is a good idea to ramp up the velocity instead of passing +- a target velocity which can be done like\ndef ramped_vel(v_prev, v_target, t_prev, t_now, ramp_rate):\n  step = ramp_rate * (t_now - t_prev).to_sec()\n  sign = 1.0 if (v_target &gt; v_prev) else -1.0\n  error = math.fabs(v_target - v_prev)\n  if error &lt; step: # we can get there within this timestep-we're done.\n  return v_target\n  else:\n  return v_prev + sign * step # take a step toward the target\n\n\ndef ramped_twist(prev, target, t_prev, t_now, ramps):\n  tw = Twist()\n  tw.angular.z = ramped_vel(prev.angular.z, target.angular.z, t_prev,\nt_now, ramps[0])\n  tw.linear.x = ramped_vel(prev.linear.x, target.linear.x, t_prev,\nt_now, ramps[1])\n  return tw\n  \ndef send_twist():\n  global g_last_twist_send_time, g_target_twist, g_last_twist,\\\ng_vel_scales, g_vel_ramps, g_twist_pub\n  t_now = rospy.Time.now()\n  g_last_twist = ramped_twist(g_last_twist,     g_target_twist,\n  g_last_twist_send_time, t_now, g_vel_ramps)\n  g_last_twist_send_time = t_now\n  g_twist_pub.publish(g_last_twist)"
  },
  {
    "objectID": "posts/SLAM in ROS/index.html#to-drive-the-robot-using-keyboard",
    "href": "posts/SLAM in ROS/index.html#to-drive-the-robot-using-keyboard",
    "title": "Implementing SLAM in ROS",
    "section": "To drive the Robot using keyboard",
    "text": "To drive the Robot using keyboard\ncd catkin_ws\nsource devel/setup.bash\n\n#terminal 1\nroslaunch turtlebot3_gazebo turtlebot3_world.launch\n\n#terminal 2\npython key_publisher.py\n\n#terminal 3\npython keys_to_twist.py"
  },
  {
    "objectID": "posts/SLAM in ROS/index.html#visualize-using-rviz",
    "href": "posts/SLAM in ROS/index.html#visualize-using-rviz",
    "title": "Implementing SLAM in ROS",
    "section": "Visualize using RVIZ",
    "text": "Visualize using RVIZ\nrviz stands for ROS visualization. It is a general purpose 3D visualization environment for robots,sensors and algorithm.\nFew fun facts about rviz\n\nrviz live in package also called rviz\nall forms of data are attached to a frame of reference. camera on turtlebot is attached to a reference frame defined relative to the center of the turtlebot’s mobile base. odom is taken where robot is powered on.\nSince all our data must be visualize wrt to a frame of reference.We must select this frame of reference in the global options/Fixed Frame\n\nTf topic primer\n\nWe must publish our frame of references relation over time on this topic.rviz also uses this topic to finds different frames. Why do we need this? because\nA robotic system typically has many 3D coordinate frames that change over time, such as a world frame, base frame, gripper frame, head frame, etc. tf keeps track of all these frames over time, and allows you to ask questions like:\n\nWhere was the head frame relative to the world frame, 5 seconds ago?\nWhat is the pose of the object in my gripper relative to my base?\nWhat is the current pose of the base frame in the map frame?\n\nAdd these line to your turtlebot 3 launch file to publish proper tf messages\n&lt;!-- Modified lines in the orgiginal package--&gt;\n\n&lt;node name=\"joint_state_publisher\" pkg=\"joint_state_publisher\" type=\"joint_state_publisher\"&gt; &lt;param name=\"use_gui\" value=\"TRUE\" /&gt; &lt;/node&gt;\n\n&lt;node name=\"robot_state_publisher\" pkg=\"robot_state_publisher\" type=\"robot_state_publisher\" /&gt;\n\n\nrviz has a no of panels and plugins that can be configured. after configuring,rviz can be saved , and loads automatically when you open it next time\nyou can add plugins like Image plugin to visualize what your camera see(you need to put the camera feed topic) or visualize"
  },
  {
    "objectID": "posts/SLAM in ROS/index.html#building-maps-of-the-world",
    "href": "posts/SLAM in ROS/index.html#building-maps-of-the-world",
    "title": "Implementing SLAM in ROS",
    "section": "Building Maps of the World",
    "text": "Building Maps of the World\nbuilding a map would be simple: you could take the objects detected by the sensors, transform them into some global coordinate frame (using the robot’s position and some geome‐try), and then record them in a map (in this global coordinate frame).\nWRONG !!!\nBecause no sensor is perfect and robot does not know with full confidence how it is moving ( supposed the wheels slipped a little)\n\n\n\n\n\n\nROSBAG\nrosbag is a tool that lets us record messages and replay them later.\nwhy use them ?\n1. debugging new algorithms, since it lets you present the same data to the algo‐ rithm over and over, which will help you isolate and fix bugs.\n\nYou can record some sensor data from the robot with rosbag ,then use this recorded data to work on your code.\n\nTo record:\n&gt;&gt; rosbag record scan tf #saves in YYYY-MM-DD-HH-mm-ss.bag\n&gt;&gt;rosbag record -O foo.bag scan tf\n#saves in foo.bag\n&gt;&gt;rosbag record -o foo scan tf #saves in foo_2015-10-05-14-29-30.bag\n&gt;&gt; rosbag record -a\nto play the saved msgs\n&gt;&gt; rosbag play --clock foo.bag\n--clock flag causes rosbag to publish clock time which ll be important when we come to build our maps\n\n\n\nwe ll build maps with the slam_gmapping node from the gmapping package. The slam_gmapping node uses an implementation of the GMapping algorithm. GMapping uses a Rao-Blackwellized particle filter to keep track of the likely positions of the robot, based on its sensor data and the parts of the map that have already been built.\nWe’re going to drive the robot around and save the sensor data to a file using rosbag . We’re then going to replay this sensor data and use slam_gmapping to build a map for us.\nto create a map,run the following\n#terminal 1 # start the turtle bot in  the world\nroslaunch turtlebot3_gazebo turtlebot3_house.launch\n\n#terminal 2: start the gmapping node. Run this either directly or record the data first in a ros bag and then run it \nroslaunch turtlebot3_slam turtlebot3_slam.launch slam_methods:=gmapping\n\n#terminal 3 : run rhe robot\nroslaunch turtlebot3_teleop turtlebot3_teleop_key.launch"
  },
  {
    "objectID": "posts/What the F is Quaternions/index.html#torch.tensor",
    "href": "posts/What the F is Quaternions/index.html#torch.tensor",
    "title": "Pytorch and TensorFlowCheat Sheet for RL",
    "section": "Torch.Tensor",
    "text": "Torch.Tensor\nPyTorch provides torch.Tensor to represent a multi-dimensional array containing elements of a single data type\nIt is basically the same as a numpy array: it does not know anything about deep learning or computational graphs or gradients, and is just a generic n-dimensional array to be used for arbitrary numeric computation. The biggest difference between a numpy array and a PyTorch Tensor is that a PyTorch Tensor can run on either CPU or GPU. To run operations on the GPU, just cast the Tensor to a cuda datatype.\nThere are three attributes : torch.dtype, torch.device and torch.layout\na = torch.rand(2,2, device='cpu')\n# transfer tensor created on cpu to gpu accessible memory.\nif torch.cuda.is_available():\n      device = torch.device('cuda') # create a device handle\n       a = a.to(device) # pass device handle created."
  },
  {
    "objectID": "posts/What the F is Quaternions/index.html#numpy-to-tensor",
    "href": "posts/What the F is Quaternions/index.html#numpy-to-tensor",
    "title": "Pytorch and TensorFlowCheat Sheet for RL",
    "section": "numpy to Tensor",
    "text": "numpy to Tensor\n&gt;&gt;&gt; print(f\"observation= {observation}\") \nobservation= [ 0.02916372 0.02608052 0.01869606 -0.00384168]\n&gt;&gt;&gt; state = T.tensor([observation],dtype=T.float).to(self.actor.device)\n&gt;&gt;&gt; print(f\"state= {state}\")\nstate= tensor([[ 0.0292,  0.0261,  0.0187, -0.0038]], device='cuda:0')\n\n\n# Another way. convert from numpy to PyTorch\nnp_array = np.ones((2,3))\npytorch_tensor = torch.from_numpy(np_array)\n# convert from PyTorch to numpy\npt_tensor = torch.rand(2,3)\nnumpy_array = pt_tensor.numpy()\n# they share same underlying memory. So, changes to one shall be reflected on the other. \n# for ex : pt_tensor update results in an update to numpy_array."
  },
  {
    "objectID": "posts/What the F is Quaternions/index.html#copying-tensor",
    "href": "posts/What the F is Quaternions/index.html#copying-tensor",
    "title": "Pytorch and TensorFlowCheat Sheet for RL",
    "section": "Copying Tensor",
    "text": "Copying Tensor"
  },
  {
    "objectID": "posts/What the F is Quaternions/index.html#save-and-load-model-checkpoint-file",
    "href": "posts/What the F is Quaternions/index.html#save-and-load-model-checkpoint-file",
    "title": "Pytorch and TensorFlowCheat Sheet for RL",
    "section": "Save and Load Model checkpoint File",
    "text": "Save and Load Model checkpoint File\nIf the NN class inherits from torch.nn.Module,then you can get the parameters/weights directly from self.state_dict() and save using torch.save()\nT.save(self.state_dict(),self.checkpoint_file)"
  },
  {
    "objectID": "posts/What the F is Quaternions/index.html#pytorch-softmax-distribution",
    "href": "posts/What the F is Quaternions/index.html#pytorch-softmax-distribution",
    "title": "Pytorch and TensorFlowCheat Sheet for RL",
    "section": "PyTorch SoftMax / Distribution",
    "text": "PyTorch SoftMax / Distribution\nwe get a tensor with Probability dist from NN(last layer was softmax). We then get an action by sampling the dist object\n&gt;&gt;&gt; dist = self.actor(state)\n&gt;&gt;&gt; print(f\"dist {dist} {dist.probs}\")\ndist Categorical(probs: torch.Size([1, 2])) tensor([[0.4913, 0.5087]], device='cuda:0', grad_fn=&lt;DivBackward0&gt;)\n\n&gt;&gt;&gt; action = dist.sample()\n&gt;&gt;&gt; print(f\"action = {action}\")\naction = tensor([1], device='cuda:0')"
  },
  {
    "objectID": "posts/What the F is Quaternions/index.html#t.squeeze",
    "href": "posts/What the F is Quaternions/index.html#t.squeeze",
    "title": "Pytorch and TensorFlowCheat Sheet for RL",
    "section": "T.squeeze()",
    "text": "T.squeeze()\ntorch.squeeze(input, dim=None) → Tensor Returns a tensor with all specified dimensions of input of size 1 removed.\nFor example, if input is of shape: (A×1×B×C×1×D) then the input.squeeze() will be of shape: (A×B×C×D).\nWhen dim is given, a squeeze operation is done only in the given dimension(s). If input is of shape: (A×1×B), squeeze(input, 0) leaves the tensor unchanged, but squeeze(input, 1) will squeeze the tensor to the shape (A×B). Parameters: input (Tensor) – the input tensor.\ndim (int or tuple of ints, optional) –if given, the input will be squeezed only in the specified dimensions\n&gt;&gt;&gt; x = torch.zeros(2, 1, 2, 1, 2) \n&gt;&gt;&gt; x.size() \ntorch.Size([2, 1, 2, 1, 2]) \n&gt;&gt;&gt; y = torch.squeeze(x) \n&gt;&gt;&gt; y.size() \ntorch.Size([2, 2, 2]) \n&gt;&gt;&gt; y = torch.squeeze(x, 0) \n&gt;&gt;&gt; y.size() \ntorch.Size([2, 1, 2, 1, 2]) \n&gt;&gt;&gt; y = torch.squeeze(x, 1) \n&gt;&gt;&gt; y.size() \ntorch.Size([2, 2, 1, 2]) \n&gt;&gt;&gt; y = torch.squeeze(x, (1, 2, 3)) \ntorch.Size([2, 2, 2])"
  },
  {
    "objectID": "posts/What the F is Quaternions/index.html#optimizer.zero_grad",
    "href": "posts/What the F is Quaternions/index.html#optimizer.zero_grad",
    "title": "Pytorch and TensorFlowCheat Sheet for RL",
    "section": "optimizer.zero_grad",
    "text": "optimizer.zero_grad"
  },
  {
    "objectID": "posts/What the F is Quaternions/index.html#total_loss.backward",
    "href": "posts/What the F is Quaternions/index.html#total_loss.backward",
    "title": "Pytorch and TensorFlowCheat Sheet for RL",
    "section": "total_loss.backward",
    "text": "total_loss.backward"
  },
  {
    "objectID": "posts/What the F is Quaternions/index.html#optimizer.step",
    "href": "posts/What the F is Quaternions/index.html#optimizer.step",
    "title": "Pytorch and TensorFlowCheat Sheet for RL",
    "section": "optimizer.step",
    "text": "optimizer.step"
  },
  {
    "objectID": "posts/What the F is Quaternions/index.html#create-a-tensorflow-neural-network",
    "href": "posts/What the F is Quaternions/index.html#create-a-tensorflow-neural-network",
    "title": "Pytorch and TensorFlowCheat Sheet for RL",
    "section": "Create a TensorFlow Neural Network",
    "text": "Create a TensorFlow Neural Network\nSet learning Rate\nIterate over weights\nsave and load weights\nnumpy to tensor\ntf.random.normal\ntf.clip by value\ntf.gradient_tape\ntape.gradient\napply_gradient\nkeras.losses.MSE"
  },
  {
    "objectID": "posts/What the F is Quaternions/index.html",
    "href": "posts/What the F is Quaternions/index.html",
    "title": "What the F is Quaternions",
    "section": "",
    "text": "If you are seeing Quaternions for the first time you probably are wondering why do we need 4 elements to represent a 3D world rotation. Wont three angles i.e. about x,y,z axis suffice. Yes and no? yes because that is what is called Euler angles and no because something called Gimbal Locks.\nExplain This\n** Why do we love quaternions in computer graphics? Because they have a number of appealing properties. First one can nicely interpolate them, which is important if one is animating rotating things, like the limbs around a joint. With a quaternion it is just scalar multiplication and normalization. Expressing this with a matrix requires evaluation of sin and cos, then building a rotation matrix. Then multiplying a vector with a quaternion is still cheaper as going through a full vector-matrix multiplication, it is also still cheaper if one adds a translation afterwards. If you consider a skeletal animation system for a human character, where one must evaluate a lot of translation/rotations for a large number of vertices, this has a huge impact. **"
  },
  {
    "objectID": "posts/Controlling NASA's R2 robot/index.html",
    "href": "posts/Controlling NASA's R2 robot/index.html",
    "title": "Controlling NASA’s R2 Robot",
    "section": "",
    "text": "In this blog we ll generate write code to control NASA robot “Robonaut2” deployed in International space station. This blog is heavily influenced from Programming Robots with ROS book from Morgan Quigley which I highly recommend.\nResult would look like this\nYou should see a R2 robot in Gazebo"
  },
  {
    "objectID": "posts/DDPG Implementation for Coders/index.html#how-actions-are-scaled-from--1-to-1-to-actual-value",
    "href": "posts/DDPG Implementation for Coders/index.html#how-actions-are-scaled-from--1-to-1-to-actual-value",
    "title": "DDPG Implementation for Coders",
    "section": "How actions are scaled from -1 to 1 to actual value",
    "text": "How actions are scaled from -1 to 1 to actual value"
  },
  {
    "objectID": "posts/Controlling NASA's R2 robot/index.html#moving-with-moveit",
    "href": "posts/Controlling NASA's R2 robot/index.html#moving-with-moveit",
    "title": "Controlling NASA’s R2 Robot",
    "section": "Moving with MoveIt",
    "text": "Moving with MoveIt\nMoveIt is a comprehensive motion planning package that interacts nicely with ros. We can give moveit target position and it does the calculation and move the robotic arm.\nTo move the arm to a random position\nstep1 : Initialize moveit\nmoveit_commander.roscpp_initialize(sys.argv)\nstep2: assign joint groups to move.Here we want to move left arm and right arm of R2 robot\ngroup = [moveit_commander.MoveGroupCommander(\"left_arm\"),moveit_commander.MoveGroupCommander(\"right_arm\")]\nstep3: create random poses.\norient = [Quaternion(*tf.transformations.quaternion_from_euler(pi,-pi/2,-pi/2)),Quaternion(*tf.transformations.quaternion_from_euler(pi,-pi/2,-pi/2))]\npose = [Pose(Point(0.5,-0.5,1.3),orient[0]),Pose(Point(-0.5,-0.5,1.3),orient[1])]\nthen in each loop change pose randomly like\npose[0].position.x = 0.5 + random.uniform(-0.1,0.1)\npose[1].position.x = -0.5 + random.uniform(-0.1,0.1)\nfor side in [0,1]:       \n  pose[side].position.z = 1.5 + random.uniform(-0.1,0.1)\nstep4: move to the given pose\ngroup[side].set_pose_target(pose[side])\ngroup[side].go(True) \n\nPlaying Chess\nlets try to make the R2 robot play chess. To do that first we need to make a model of chessboard and chess pieces for gazebo. how do we make model? SDF (simulation description file). First lets model chess pieces. We ll model them as same box but we could have imported exact mesh just as easily.\nAny SDF model have these crucial elements\n\nModel Name : name of your model for eg. “piece”\n1.1. link name: name of our link.Each model can have many links for eg.link1\n1.1.1. Intertial: details like mass,moment of interia\n1.1.2. Collision: geometry of the link for collision(may or may not be the actual mesh),friction,contact\n1.1.3. Visual : actual rendered geometry of the box\n\nLook in the Model folder for the sdf files of ChessBoard and chess Pieces. Chessboard is made static so it does not required inertial values\nNext step is to use this SDF model to put model in gazebo. We can do it using a python script. We use two gazebo services called gazebo/delete_model for deleting an existing model. Mostly used for resetting existing model and gazebo/spawn_sdf_model to spawn a model from sdf file. Here is the code for how to use it.\n#first wait for these gazebo services to be available\n\n  rospy.wait_for_service(\"gazebo/delete_model\")\n  rospy.wait_for_service(\"gazebo/spawn_sdf_model\")\n  \n#get a object wrapper for this services\n    delete_model = rospy.ServiceProxy(\"gazebo/delete_model\",DeleteModel)\n    s = rospy.ServiceProxy(\"gazebo/spawn_sdf_model\",SpawnModel)\n    \n#to spawn a model: Read xml and create an object\n    with open(\"chessboard.sdf\",\"r\") as f:\n        board_xml = f.read()\n    with open(\"chess_piece.sdf\",\"r\") as f:\n        piece_xml = f.read()\n#get the pose of the board\n    orient = Quaternion(*tf.transformations.quaternion_from_euler(0,0,0))\n    board_pose = Pose(Point(0.25,1,39,0.90),orient)\n#spawn the board\n    s(\"chessboard\",board_xml,\"\",board_pose,\"world\")\n\n#get the pose of each indivisual pieces\n            pose = deepcopy(board_pose)\n            pose.position.x = board_pose.position.x-3.5*unit+col*unit\n            pose.position.y = board_pose.position.y-3.5*unit+row*unit\n            pose.position.z+=0.02\n\n#spwan the pieces\n    piece_name = \"piece%d_%d\"%(row,col)\n    s(piece_name,piece_xml,\"\",pose,\"world\")\n\n\n#you can delete the models like\n    delete_model(\"chessboard\")\n    delete_model(piece_name)"
  },
  {
    "objectID": "posts/NLPTrajOPt/index.html",
    "href": "posts/NLPTrajOPt/index.html",
    "title": "Non Linear Trajectory Optimization:Acrobat or Double Pendulum",
    "section": "",
    "text": "Double pendulum have highly non linear dymanics. Thats why making controlling them and making them reach a desired state is an interesting control problem. In this blog we ll try to make a double pendulum stand straight by controlling a torque only at the mid joint using multiple shooting method. The result would look something like this.\n\n\nWhy it is called the Acrobat? Because Acrobats does not have enough torque at their wrist. So they swing their body to reach vertical position above the pole\n\nDynamics Equation\nFor a double pendulum with only torque \\(u\\) at the middle joint the dynamics is given by\n\\(M(q)\\ddot q + C(q,\\dot q)\\dot q= \\tau_g(q)+Bu\\)\nwhere\n$ M(q) =\n\\[\\begin{matrix}I_1+I_2+m_2l_1^2+2m_2l_1l_2c_2 & I_2+m_2l_1l_2c_2\\\\I_2+m_2l_1l_2c_2 & 1_2 \\end{matrix}\\]\n$\n\\(C(q,\\dot q) = \\begin{matrix} -2m_2l_1l_2s_2\\dot q_2 & -m_2l_1l_2s_2\\dot q_2 \\\\ m_2l_1l_2s_2\\dot q_1 & 0\\end{matrix}\\)\n\\(\\tau_g(q) = \\begin{matrix} -m_1gl_1s_1-m_2g(l_1s_1+l_2s_{1+2}) \\\\ -m_2gl_2s_{1+2} \\end{matrix}\\)\n\\(B = \\begin{matrix} 0 \\\\ 1\\end{matrix}\\)\nwhere\n\\(q = [\\theta_1,\\theta_2]\\)\n\\(s1 = sin(\\theta_1) ,\\ s_{1+2} = sin(\\theta_1+\\theta_2)\\)\nwe can code this in python like\ndef deriv2(y,u):\n    theta1 = y[0]\n    theta2 = y[1]\n    y1_dot = y[2]\n    y2_dot = y[3]\n    g = 9.81\n    s1 = np.sin(theta1)\n\n\n    M = np.array([[I1+I2+m2*l1**2+2*m2*l1*l2*np.cos(theta2),I2+m2*l1*l2*np.cos(theta2)],\n         [I2+m2*l1*l2*np.cos(theta2),I2]])\n    print(M.shape)\n    C = np.array([[-2*m2*l1*l2*np.sin(theta2)*y2_dot, -m2*l1*l2*np.sin(theta2)*y2_dot],\n         [m2*l1*l2*np.sin(theta2)*y1_dot,0]])\n    print(C.shape)\n    T = np.array([[-m1*g*l1*s1-m2*g*(l1*s1+l2*np.sin(theta1+theta2))],[-m2*g*l2*np.sin(theta1+theta2)]])\n    print(T.shape)\n    B = np.array([[0],[1]])\n    print(B.shape)\n    d2y = np.matmul(np.linalg.inv(M),T+np.matmul(B,u)-np.matmul(C,[[y1_dot],[y2_dot]]))\n    return np.array([y1_dot,y2_dot,d2y[0][0],d2y[1][0]])\n\n\nOptimal Equation Formulation\nwe use multiple shooting method that means we put state and the control input u as decision variable and use the dynamics equation as constraints\nwe want to minimize the cost function\n L =  0.5*(u)**2 + 100*(MX.cos(theta1)+1)**2+100*(MX.cos(theta2)+1)**2+10*dtheta1**2+10*dtheta2**2\nsubject to\n\\(\\dot x-f(x,u)=0\\)\nor if we discretize using RK4 at \\(t_n\\) we get\n\\(X_{n+1} -g(X_{n},u)=0\\)\n\n\nCode\nwe use CASADI as the program to solve this NLP. I recommend watching this tutorial video from the creater of Casadi,Joris Gillis to know how to use it https://www.youtube.com/watch?v=ANicKS8gKdM\nfirst create the dynamics in casadi matrix form\ndef deriv(y,t,u,l):\n    theta1 = y[0]\n    theta2 = y[1]\n    y1_dot = y[2]\n    y2_dot = y[3]\n    g = 9.81\n    s1 = MX.sin(theta1)\n\n    M = MX(2,2)\n    M[0,0] = I1+I2+m2*l1**2+2*m2*l1*l2*MX.cos(theta2)\n    M[0,1] = I2+m2*l1*l2*MX.cos(theta2)\n    M[1,0] = I2+m2*l1*l2*MX.cos(theta2)\n    M[1,1] = I2\n    \n    C = MX(2,2)\n    C[0,0] = -2*m2*l1*l2*MX.sin(theta2)*y2_dot\n    C[0,1] = -m2*l1*l2*MX.sin(theta2)*y2_dot\n    C[1,0] = m2*l1*l2*MX.sin(theta2)*y1_dot\n    C[1,1] = 0\n\n    T = MX(2,1)\n    T[0,0] = -m1*g*l1*s1-m2*g*(l1*s1+l2*MX.sin(theta1+theta2))\n    T[1,0] = -m2*g*l2*MX.sin(theta1+theta2)\n   \n    B = MX(2,1)\n    B[0,0] = 0\n    B[1,0] = 1\n    \n    y_dot = MX(2,1)\n    y_dot[0,0]=y1_dot \n    y_dot[1,0] = y2_dot\n\n    d2y = solve(M,T+mtimes(B,u)-mtimes(C,y_dot))\n    return y1_dot,y2_dot,d2y[0][0],d2y[1][0]\nThen define and set the optimal control problem\nx_k = state at Kth time step; 4x1 matrix\nu_k = torque at Kth time step; 1x1 matrix\nlbw = lower limit on decision variable\nubw = upper limit on decision variable\nw0 = initial guess on the state\nlbg = constraints lower limit\nubg = constraints upper limit\nw=[]\nw0 = []\nlbw = []\nubw = []\nJ = 0\ng=[]\nlbg = []\nubg = []\n\n# \"Lift\" initial conditions\nXk = MX.sym('X0', 4)\nw += [Xk]\nlbw += [0.1, 0.0,0,0]\nubw += [0.1, 0.0,0,0]\nw0 += [0.1, 0.0,0,0]\n\n# Formulate the NLP\nfor k in range(N):\n    # New NLP variable for the control\n    Uk = MX.sym('U_' + str(k))\n    w   += [Uk]\n    lbw += [-inf]\n    ubw += [inf]\n    w0  += [0]\n\n    # Integrate till the end of the interval\n    Fk = F(x0=Xk, u=Uk)\n    Xk_end = Fk['xf']\n    J=J+Fk['qf']\n\n    # New NLP variable for state at end of interval\n    Xk = MX.sym('X_' + str(k+1), 4)\n    w   += [Xk]\n    lbw += [-inf, -inf,-inf,-inf]\n    ubw += [ inf,inf,inf,inf]\n    w0  += [0,0,0,0]\n\n    # Add equality constraint\n    g   += [Xk_end-Xk]\n    lbg += [0, 0,0,0]\n    ubg += [0, 0,0,0]\n\n# Create an NLP solver\nprob = {'f': J, 'x': vertcat(*w), 'g': vertcat(*g)}\nsolver = nlpsol('solver', 'ipopt', prob)\n\n\nsol = solver(x0=w0, lbx=lbw, ubx=ubw, lbg=lbg, ubg=ubg)\nw_opt = sol['x'].full().flatten()\n\n# Plot the solution\nx1_opt = w_opt[0::5]\nx2_opt = w_opt[1::5]\nx3_opt = w_opt[2::5]\nx4_opt = w_opt[3::5]\nu_opt = w_opt[4::5]\n\n\nAnimate optimal states\nfig = plt.figure(figsize=(8.33,6.25),dpi=72)\nax = fig.add_subplot(111)\ny = [math.pi/3,math.pi/2,0,0]\nx1 = math.sin(y[0])\ny1 = - math.cos(y[0])\nx2 = x1+math.sin(y[1])\ny2 = y1-math.cos(y[1])\nline, = ax.plot([0,x1,x2],[0,y1,y2],lw=2,c='k')\nr=0.05\nc1 = Circle((x1,y1),r,fc='b',ec='b',zorder=10)\nc2 = Circle((x2,y2),r,fc='b',ec='b',zorder=10)\ncircle1 = ax.add_patch(c1)\ncircle2  = ax.add_patch(c2)\nns=20\nax.set_xlim(-5, 5)\nax.set_ylim(-5, 5)\n\n\n\ndef animate(i):\n    x1 = math.sin(y_track[i,0])\n    y1 = - math.cos(y_track[i,0])\n    x2 = x1+math.sin(y_track[i,1])\n    y2 = y1-math.cos(y_track[i,1])\n    line.set_data([0,x1,x2],[0,y1,y2])\n    circle1.set_center((x1,y1))\n    circle2.set_center((x2,y2))\n    \ny_init = [0.1,0,0,0]\nX = y_init\nt = [0,0.1]\n\ny_track=np.array([y_init for i in range(100)])\nprint(y_track.shape)\nfor i in range(0,99):\n    y_track[i,:] = [x1_opt[i],x2_opt[i],x3_opt[i],x4_opt[i]]\n    \n\nani = animation.FuncAnimation(fig, animate,frames=99,interval=100)\n#plt.show()\nFFwriter = animation.FFMpegWriter(fps=10)\nani.save('animation.mp4', writer = FFwriter)"
  }
]