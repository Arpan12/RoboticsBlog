---
title: "PPO Implementation for Coders"
author: "Arpan Pallar"
date: "2023-06-10"
categories: [RL, Code, Python]
image: "image.jpg"
---

PPO is state of the art algorithm for DeepRL. We ll see how this can be implemented in code.

We wont go in depth into theory but focus on implementation.

### Promise

After reading till the end of the blog, you should be able to implement PPO for **discrete action space** based environments.

### Theory

PPO is a policy gradient method

$\pi(u|x)\to$our current policy. **This in deep RL is usually approximate by a Neural Net called actor net**

$g(x_n,u_n)\to$ cost incurred, we get from the environment for taking action

$V_\pi(x) = g(x,\pi(x)) + \alpha*V_\pi(x+1)$ = value function for policy $\pi$ . **This in deep RL is usually approximate by a Neural Net called Critic net**

if $J(\theta) = V_\pi(x_0)$ approximate value function for an episode starting at x0 following policy $\pi(\theta)$ ,We can decrease $J(\theta)$ by gradient descent $\theta_{new} = \theta - \gamma*\Delta J(\theta)$

so writing more mathematically

$J(\theta) = E_{u_n}\sim \pi_{\theta}[\sum_{n=0}^N\alpha^n*g(x_n,u_n)]$

we are able to show(proof not important)

$\Delta_\theta J(\theta) = E[\sum_{n=0}^N\Psi_n\Delta_nlog\ \pi(u_n|x_n,\theta)]$

which is similar to minimizing:

$min_\theta E[\Psi_n*log\ \pi(u_n|x_n,|\theta)]$

**This is what we would use for actor loss and minimize. In Practice since we get rewards from the Env so we put a negative sign before** $\Psi_n$ **so that we maximize the reward gain**

**For critic loss we take** $\Delta t_d^2$ where $t_d = g(x_n,u_n)+\gamma*V(x_{n+1)}-V(x_n)$

for

1.  ReInforce: $\Psi_n = G_n = \sum_{k=n}^N \alpha^k * g(x_k,u_k)$

2.  ReInforce with baseLine: $\Psi_n = G_n-V(x_n)$

3.  Actor-Critic: $\Psi_n = g(x_n,u_n)+\alpha*V(x_{n+1})-V(x_n)$

4.  PPO: $\Psi_n = A_t = \sum_{t=k}^N (\lambda *\mu)^{t-k}*( Q(x_n,u_n)-V(x_n))$

    where $Q(x_n,u_n) = g(x_n,u_n)+\gamma*V({x_{n+1}})$

    but for PPO we approximate the log gradient even further by $min_\theta E[\sum_{n=0}^N A_n\frac{\pi(u_n|x_n,Theta)}{\pi(u_n| x_n,\theta_{old})}]$

For PPO we generally clip the gradient by limiting between 1-$\epsilon$ and 1+$\epsilon$

$min_\theta E[\sum_{n=0}^N min(A_n\ \frac{\pi(u_n|x_n,Theta)}{\pi(u_n| x_n,\theta_{old}},clip(\frac{\pi(u_n|x_n,Theta)}{\pi(u_n| x_n,\theta_{old}},1-\epsilon,1+\epsilon)A_n)]$

issue to address:

```{r}
1 + 1
```

### Implementation

Batches and How to generate them

compute the advantage

loss function
