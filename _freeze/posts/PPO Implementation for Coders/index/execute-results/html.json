{
  "hash": "97112490ce8e6b2e9de54341e31eb0a1",
  "result": {
    "markdown": "---\ntitle: \"PPO Implementation for Coders\"\nauthor: \"Arpan Pallar\"\ndate: \"2023-06-10\"\ncategories: [RL, Code, Python]\nimage: \"image.jpg\"\n---\n\n\nPPO is state of the art algorithm for DeepRL. We ll see how this can be implemented in code.\n\nWe wont go in depth into theory but focus on implementation.\n\n### Promise\n\nAfter reading till the end of the blog, you should be able to implement PPO for **discrete action space** based environments.\n\n### Theory\n\nPPO is a policy gradient method\n\n$\\pi(u|x)\\to$our current policy. **This in deep RL is usually approximate by a Neural Net called actor net**\n\n$g(x_n,u_n)\\to$ cost incurred, we get from the environment for taking action\n\n$V_\\pi(x) = g(x,\\pi(x)) + \\alpha*V_\\pi(x+1)$ = value function for policy $\\pi$ . **This in deep RL is usually approximate by a Neural Net called Critic net**\n\nif $J(\\theta) = V_\\pi(x_0)$ approximate value function for an episode starting at x0 following policy $\\pi(\\theta)$ ,We can decrease $J(\\theta)$ by gradient descent $\\theta_{new} = \\theta - \\gamma*\\Delta J(\\theta)$\n\nso writing more mathematically\n\n$J(\\theta) = E_{u_n}\\sim \\pi_{\\theta}[\\sum_{n=0}^N\\alpha^n*g(x_n,u_n)]$\n\nwe are able to show(proof not important)\n\n$\\Delta_\\theta J(\\theta) = E[\\sum_{n=0}^N\\Psi_n\\Delta_nlog\\ \\pi(u_n|x_n,\\theta)]$\n\nwhich is similar to minimizing:\n\n$min_\\theta E[\\Psi_n*log\\ \\pi(u_n|x_n,|\\theta)]$\n\n**This is what we would use for actor loss and minimize. In Practice since we get rewards from the Env so we put a negative sign before** $\\Psi_n$ **so that we maximize the reward gain**\n\n**For critic loss we take** $\\Delta t_d^2$ where $t_d = g(x_n,u_n)+\\gamma*V(x_{n+1)}-V(x_n)$\n\nfor\n\n1.  ReInforce: $\\Psi_n = G_n = \\sum_{k=n}^N \\alpha^k * g(x_k,u_k)$\n\n2.  ReInforce with baseLine: $\\Psi_n = G_n-V(x_n)$\n\n3.  Actor-Critic: $\\Psi_n = g(x_n,u_n)+\\alpha*V(x_{n+1})-V(x_n)$\n\n4.  PPO: $\\Psi_n = A_t = \\sum_{t=k}^N (\\lambda *\\mu)^{t-k}*( Q(x_n,u_n)-V(x_n))$\n\n    where $Q(x_n,u_n) = g(x_n,u_n)+\\gamma*V({x_{n+1}})$\n\n    but for PPO we approximate the log gradient even further by $min_\\theta E[\\sum_{n=0}^N A_n\\frac{\\pi(u_n|x_n,Theta)}{\\pi(u_n| x_n,\\theta_{old})}]$\n\nFor PPO we generally clip the gradient by limiting between 1-$\\epsilon$ and 1+$\\epsilon$\n\n$min_\\theta E[\\sum_{n=0}^N min(A_n\\ \\frac{\\pi(u_n|x_n,Theta)}{\\pi(u_n| x_n,\\theta_{old}},clip(\\frac{\\pi(u_n|x_n,Theta)}{\\pi(u_n| x_n,\\theta_{old}},1-\\epsilon,1+\\epsilon)A_n)]$\n\nissue to address:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 + 1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n```\n:::\n:::\n\n\n### Implementation\n\nBatches and How to generate them\n\ncompute the advantage\n\nloss function\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}