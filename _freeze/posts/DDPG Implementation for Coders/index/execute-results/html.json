{
  "hash": "33963fcac80dda1fdb4274e8401d60ef",
  "result": {
    "markdown": "---\ntitle: \"PPO Implementation for Coders\"\nauthor: \"Arpan Pallar\"\ndate: \"2023-06-10\"\ncategories: [RL, Code, Python]\nimage: \"image.jpg\"\n---\n\n\nDDPG is a deepRL algorithm for agents with continuous actions space and Useful Particularly in the field of robotics where actuators and motors take continuous signal . We ll see how this can be implemented in code.\n\nWe wont go in depth into theory but focus on implementation.\n\n# Promise\n\nAfter reading till the end of the blog, you should be able to implement DDPG for **continuous action space** based environments.\n\n# Theory\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 + 1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n```\n:::\n:::\n\n\n# Implementation\n\n## Make Actor and Critic NN\n\n**actor NN**: takes in a state(you need to make a tensor out of your state.Cant pass a list or np array) and spits out a Categorical object containing probs(because the last layer was softmax on action_dim)\n\nfor eg: in our case our observation is (4,) python list\n\n```         \n>>> print(f\"observation= {observation}\")\nobservation= [ 0.02916372  0.02608052  0.01869606 -0.00384168]\n```\n\nwe need to convert them to Tensor before passing them to NN\n\n```         \n>>> state = T.tensor([observation],dtype=T.float).to(self.actor.device)\n>>> print(f\"state= {state}\")\nstate= tensor([[ 0.0292,  0.0261,  0.0187, -0.0038]], device='cuda:0')\n```\n\nwe get a tensor with Probability dist from NN(last layer was softmax). We then get an action by sampling the dist object\n\n```         \n>>> dist = self.actor(state)\n>>> print(f\"dist {dist} {dist.probs}\")\ndist Categorical(probs: torch.Size([1, 2])) tensor([[0.4913, 0.5087]], device='cuda:0', grad_fn=<DivBackward0>)\n\n>>> action = dist.sample()\n>>> print(f\"action = {action}\")\naction = tensor([1], device='cuda:0')\n\n# Retrieve action from action tensor and log_probs\n# squeeze removes all axises having value 1. item() returns a standard python float\n>>>  probs = T.squeeze(dist.log_prob(action)).item()\n>>>  action = T.squeeze(action).item()\n>>> print(f\"log prob {dist.log_prob(action)} squeezed = {T.squeeze(dist.log_prob(action))} Probs = {probs}\")\nlog prob tensor([-0.7533], device='cuda:0', grad_fn=<SqueezeBackward1>) squeezed = -0.753328263759613 Probs = -0.753328263759613\n```\n\nThe code For Actor class. It derives from Torch.nn.Module\n\n```         \nclass AgentNetwork(nn.Module):\n    def __init__(self,input_dims,action_dim,lr,layer1=256,layer2=256,weight_file='weightFiles/ppo_discrete'):\n        super(AgentNetwork,self).__init__()\n        self.checkpoint_file = os.path.join(weight_file,'ppo_actor_weight')\n        #TOCHECK: *input_dims vs input_dims\n        self.actor = nn.Sequential(\n                nn.Linear(*input_dims,layer1),\n                nn.ReLU(),\n                nn.Linear(layer1,layer2),\n                nn.ReLU(),\n                nn.Linear(layer2,action_dim),\n                nn.Softmax(dim=-1)               \n        )\n        \n        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n        self.to(self.device)\n    \n    def forward(self,state):\n        dist = self.actor(state)\n        #TOCHECK: what does categorical do\n        dist = Categorical(dist)\n        return dist\n    \n    def save_checkpoint(self):\n        T.save(self.state_dict(),self.checkpoint_file)\n    \n    def load_checkpoint(self):\n        self.load_state_dict(T.load(self.checkpoint_file))\n```\n\n**Critic NN**: takes in a state(tensor) and returns a tensor containing a single float corresponding to Value of that state\n\n```         \n>>> state = T.tensor([observation],dtype=T.float).to(self.actor.device)\n>>> value = self.critic(state)\n>>> print(f\"value {value}\")\nvalue tensor([[-0.0977]], device='cuda:0', grad_fn=<AddmmBackward0>)\n```\n\nCode for Critic Class\n\n```         \n class CriticNetwork(nn.Module):\n    def __init__(self,input_dims,lr,layer1=256,layer2=256,weight_file='weightFiles/ppo_discrete'):\n        super(CriticNetwork,self).__init__()\n        self.checkpoint_file = os.path.join(weight_file,'ppo_critic_weight')\n        self.critic = nn.Sequential(\n                nn.Linear(*input_dims,layer1),\n                nn.ReLU(),\n                nn.Linear(layer1,layer2),\n                nn.ReLU(),\n                nn.Linear(layer2,1)\n        )\n        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n        self.to(self.device)\n        \n    def forward(self,state):\n        value = self.critic(state)\n        return value\n    \n    def save_checkpoint(self):\n        T.save(self.state_dict(),self.checkpoint_file)\n    \n    def load_checkpoint(self):\n        self.load_state_dict(T.load(self.checkpoint_file))\n```\n\n## Batches and How to generate them\n\nWe need to take all the elements in the memory and create batches out of them.\n\n**GOAL**\n\n1.The elements should not repeat from one batch to another\n\n2.There should be any correlation between elements of same batch.\n\nHere is how we achieve that in code\n\n```         \n  def generate_batches(self):\n        n_states = len(self.states)\n        batches=[]\n        i=0\n        indices = np.arange(n_states,dtype = np.int64)\n        np.random.shuffle(indices)\n        for i in range(n_states):\n            batches.append(indices[i:i+self.batch_size])\n            i+=self.batch_size\n        return np.array(self.states),\\\n                np.array(self.actions),\\\n                np.array(self.probs), \\\n                np.array(self.vals),\\\n                np.array(self.rewards), \\\n                np.array(self.dones),\\\n                batches\n#Note here only batch index get suffled. We need to have the actual order of states,actions,probs,vals,rewards and done so that we can calculate advatage of each state later\n```\n\n## compute the advantage\n\nWe know that advantage of a state is given by following equation\n\n$A_t = \\sum_{t=k}^N (\\lambda *\\mu)^{t-k}*( Q(x_n,u_n)-V(x_n))$\n\nHere is how we achieve that in code\n\n```         \nstate_arr,action_arr,probs_arr,vals_arr, \\\n    rewards_arr,dones_arr,batches = self.memory.generate_batches()\n            \nadvantages=np.zeros_like(rewards_arr)\n            \nfor t in reversed(range(len(state_arr)-1)):\n    advantages[t] = rewards_arr[t]+self.gamma*vals_arr[t+1]*(1-int(dones_arr[t]))-vals_arr[t] + self.gamma*self.lambda_factor*advantages[t+1]\n                \nadvantages = T.tensor(advantages).to(self.actor.device)\n```\n\n## loss function\n\nfor **actor loss**,we know that we need to minimize\n\n$min_\\theta E[\\sum_{n=0}^N min(A_n\\ \\frac{\\pi(u_n|x_n,Theta)}{\\pi(u_n| x_n,\\theta_{old}},clip(\\frac{\\pi(u_n|x_n,Theta)}{\\pi(u_n| x_n,\\theta_{old}},1-\\epsilon,1+\\epsilon)A_n)]$\n\nso we can find gradient of this function with respect to our weights $\\Theta$ and do a 1 step gradient descent\n\nThis can be done in Code by\n\n```         \n states = T.tensor(state_arr[batch],dtype = T.float).to(self.actor.device)\nactions = T.tensor(action_arr[batch],dtype = T.float).to(self.actor.device)\nold_probs = T.tensor(probs_arr[batch],dtype = T.float).to(self.actor.device)\ndist = self.actor(states)\nnew_probs = dist.log_prob(actions)\n#TOCHECK: what do exp() do\n                \nprob_ratio = new_probs.exp()/old_probs.exp()\n                \nweighted_prob = advantages[batch]*(prob_ratio)\n                \nweighted_clipped_probs = T.clamp(prob_ratio,1-self.policy_clip,1+self.policy_clip)*advantages[batch]\n                \nactor_loss = - T.min(weighted_clipped_probs,weighted_prob).mean()\n```\n\nfor **Critic loss**,we know that we need to minimize the error of values of our state $(V_{des} - V_{pred})^2$\n\nwhere\n\n$V_{des} = a_t+value_{theta\\_old}$\n\n$V_{pred} = value \\ from \\ critic \\ NN$\n\nThis is done in code as\n\n```         \ncritic_values = T.squeeze(critic_values)\n                \ndesired_state_values = advantages[batch]+values[batch]\ncritic_loss = (desired_state_values-critic_values)**2\ncritic_loss = critic_loss.mean()\n```\n\nThen We calculate the total loss and do 1 step gradient descent as\n\n```         \n total_loss = actor_loss+critic_loss*0.5\n                \nself.actor.optimizer.zero_grad()\nself.critic.optimizer.zero_grad()\ntotal_loss.backward()\nself.actor.optimizer.step()\nself.critic.optimizer.step()\n```\n\n## Learning Pseudo Code\n\n```         \nPPO_agent_learn:\n  for _ in num_of_epoch:\n    compute advantage of the states in memory\n    generate batchs\n    Iterate over batches:\n      calculate cummulative actor loss for the batch\n      calculate cummulative critic loss for the batch\n      do gradient update\n  clear Memory\n```\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}